{"config":{"lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Content.","title":"Home"},{"location":"inference/checklist.html","text":"Integration checklist \u00b6 Todo.","title":"Integration checklist"},{"location":"inference/checklist.html#integration-checklist","text":"Todo.","title":"Integration checklist"},{"location":"inference/hls4ml.html","text":"Direct inference with hls4ml \u00b6 Todo.","title":"hls4ml"},{"location":"inference/hls4ml.html#direct-inference-with-hls4ml","text":"Todo.","title":"Direct inference with hls4ml"},{"location":"inference/integrations.html","text":"Successful integrations \u00b6 Todo.","title":"Successful integrations"},{"location":"inference/integrations.html#successful-integrations","text":"Todo.","title":"Successful integrations"},{"location":"inference/onnx.html","text":"Direct inference with ONNX \u00b6 Todo.","title":"ONNX"},{"location":"inference/onnx.html#direct-inference-with-onnx","text":"Todo.","title":"Direct inference with ONNX"},{"location":"inference/performance.html","text":"Performance of inference tools \u00b6","title":"Performance"},{"location":"inference/performance.html#performance-of-inference-tools","text":"","title":"Performance of inference tools"},{"location":"inference/sonic_triton.html","text":"Service-based inference with Triton/Sonic \u00b6 Todo.","title":"Sonic/Triton"},{"location":"inference/sonic_triton.html#service-based-inference-with-tritonsonic","text":"Todo.","title":"Service-based inference with Triton/Sonic"},{"location":"inference/tensorflow1.html","text":"Direct inference with TensorFlow 1 \u00b6 Todo.","title":"TensorFlow 1"},{"location":"inference/tensorflow1.html#direct-inference-with-tensorflow-1","text":"Todo.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow2.html","text":"Direct inference with TensorFlow 2 \u00b6 TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions. Software setup \u00b6 To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml Saving your model \u00b6 After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph . Inference in CMSSW \u00b6 The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended . CMSSW module setup \u00b6 If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export> Single-threaded inference \u00b6 Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ... 2. Initialize objects \u00b6 1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef ); 3. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 4. Cleanup \u00b6 1 2 tensorflow :: closeSession ( session ); delete graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Multi-threaded inference \u00b6 Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h . 2. Define and use the cache \u00b6 The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details. 3. Initialize objects \u00b6 1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef ); 4. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 5. Cleanup \u00b6 1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Optimization \u00b6 Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist . Reusing tensors \u00b6 In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call. Tensor data access via pointers \u00b6 As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } } Inter- and intra-operation parallelism \u00b6 Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" ); Miscellaneous \u00b6 Logging \u00b6 By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet. Links and further reading \u00b6 cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"TensorFlow 2"},{"location":"inference/tensorflow2.html#direct-inference-with-tensorflow-2","text":"TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions.","title":"Direct inference with TensorFlow 2"},{"location":"inference/tensorflow2.html#software-setup","text":"To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml","title":"Software setup"},{"location":"inference/tensorflow2.html#saving-your-model","text":"After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph .","title":"Saving your model"},{"location":"inference/tensorflow2.html#inference-in-cmssw","text":"The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended .","title":"Inference in CMSSW"},{"location":"inference/tensorflow2.html#cmssw-module-setup","text":"If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export>","title":"CMSSW module setup"},{"location":"inference/tensorflow2.html#single-threaded-inference","text":"Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup.","title":"Single-threaded inference"},{"location":"inference/tensorflow2.html#1-includes","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ...","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-initialize-objects","text":"1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef );","title":"2. Initialize objects"},{"location":"inference/tensorflow2.html#3-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"3. Inference"},{"location":"inference/tensorflow2.html#4-cleanup","text":"1 2 tensorflow :: closeSession ( session ); delete graphDef ;","title":"4. Cleanup"},{"location":"inference/tensorflow2.html#full-example","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#multi-threaded-inference","text":"Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup.","title":"Multi-threaded inference"},{"location":"inference/tensorflow2.html#1-includes_1","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h .","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-define-and-use-the-cache","text":"The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details.","title":"2. Define and use the cache"},{"location":"inference/tensorflow2.html#3-initialize-objects","text":"1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef );","title":"3. Initialize objects"},{"location":"inference/tensorflow2.html#4-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"4. Inference"},{"location":"inference/tensorflow2.html#5-cleanup","text":"1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ;","title":"5. Cleanup"},{"location":"inference/tensorflow2.html#full-example_1","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#optimization","text":"Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist .","title":"Optimization"},{"location":"inference/tensorflow2.html#reusing-tensors","text":"In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call.","title":"Reusing tensors"},{"location":"inference/tensorflow2.html#tensor-data-access-via-pointers","text":"As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } }","title":"Tensor data access via pointers"},{"location":"inference/tensorflow2.html#inter-and-intra-operation-parallelism","text":"Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" );","title":"Inter- and intra-operation parallelism"},{"location":"inference/tensorflow2.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"inference/tensorflow2.html#logging","text":"By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet.","title":"Logging"},{"location":"inference/tensorflow2.html#links-and-further-reading","text":"cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"Links and further reading"},{"location":"inference/xgboost.html","text":"Direct inference with XGBoost \u00b6 General \u00b6 XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session). Existing Examples \u00b6 There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment. Example: Classification of points from joint-Gaussian distribution. \u00b6 In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv . Preparing Model \u00b6 The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details. C/C++ Usage with CMSSW \u00b6 To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following. Module setup \u00b6 There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so Basic Usage of C API \u00b6 In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */ Full Example \u00b6 Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample ) Python Usage \u00b6 To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section. Caveat \u00b6 It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ). Appendix: Tips for XGBoost users \u00b6 Importance Plot \u00b6 XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ). ROC Curve and AUC \u00b6 The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below Reference of XGBoost \u00b6 XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"XGBoost"},{"location":"inference/xgboost.html#direct-inference-with-xgboost","text":"","title":"Direct inference with XGBoost"},{"location":"inference/xgboost.html#general","text":"XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session).","title":"General"},{"location":"inference/xgboost.html#existing-examples","text":"There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment.","title":"Existing Examples"},{"location":"inference/xgboost.html#example-classification-of-points-from-joint-gaussian-distribution","text":"In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv .","title":"Example: Classification of points from joint-Gaussian distribution."},{"location":"inference/xgboost.html#preparing-model","text":"The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details.","title":"Preparing Model"},{"location":"inference/xgboost.html#cc-usage-with-cmssw","text":"To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following.","title":"C/C++ Usage with CMSSW"},{"location":"inference/xgboost.html#module-setup","text":"There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so","title":"Module setup"},{"location":"inference/xgboost.html#basic-usage-of-c-api","text":"In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */","title":"Basic Usage of C API"},{"location":"inference/xgboost.html#full-example","text":"Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample )","title":"Full Example"},{"location":"inference/xgboost.html#python-usage","text":"To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section.","title":"Python Usage"},{"location":"inference/xgboost.html#caveat","text":"It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ).","title":"Caveat"},{"location":"inference/xgboost.html#appendix-tips-for-xgboost-users","text":"","title":"Appendix: Tips for XGBoost users"},{"location":"inference/xgboost.html#importance-plot","text":"XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ).","title":"Importance Plot"},{"location":"inference/xgboost.html#roc-curve-and-auc","text":"The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below","title":"ROC Curve and AUC"},{"location":"inference/xgboost.html#reference-of-xgboost","text":"XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"Reference of XGBoost"},{"location":"optimization/introduction.html","text":"Model optimization \u00b6 Todo.","title":"Model optimization"},{"location":"optimization/introduction.html#model-optimization","text":"Todo.","title":"Model optimization"},{"location":"starter/introduction.html","text":"Starter guide introdction \u00b6 Todo.","title":"Introduction"},{"location":"starter/introduction.html#starter-guide-introdction","text":"Todo.","title":"Starter guide introdction"}]}