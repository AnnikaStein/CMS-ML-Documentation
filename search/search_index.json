{"config":{"lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Content.","title":"Home"},{"location":"inference/hls4ml.html","text":"Direct inference with hls4ml \u00b6 Todo.","title":"hls4ml"},{"location":"inference/hls4ml.html#direct-inference-with-hls4ml","text":"Todo.","title":"Direct inference with hls4ml"},{"location":"inference/onnx.html","text":"Direct inference with ONNX \u00b6 Todo.","title":"ONNX Runtime"},{"location":"inference/onnx.html#direct-inference-with-onnx","text":"Todo.","title":"Direct inference with ONNX"},{"location":"inference/performance.html","text":"Performance of Inference tools \u00b6","title":"Performance"},{"location":"inference/performance.html#performance-of-inference-tools","text":"","title":"Performance of Inference tools"},{"location":"inference/sonic_triton.html","text":"Direct inference with TensorFlow 1 \u00b6 Todo.","title":"Sonic/Triton"},{"location":"inference/sonic_triton.html#direct-inference-with-tensorflow-1","text":"Todo.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow1.html","text":"Direct inference with TensorFlow 1 \u00b6 Todo.","title":"TensorFlow 1"},{"location":"inference/tensorflow1.html#direct-inference-with-tensorflow-1","text":"Todo.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow2.html","text":"Direct inference with TensorFlow 2 \u00b6 TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.2. See the guide on inference with TensorFlow 1 or earlier versions. Software setup \u00b6 To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version, you can simply do that via pip # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --install-option = \"--prefix=CUSTOM_DIRECTORY\" cmsml to a location of your choice. Saving your model \u00b6 After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) tf.function Let's consider you write your network model in a single tf.function : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"output\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type of shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This attaches a graph object to model but disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph . Inference in CMSSW \u00b6 CMSSW module setup \u00b6 If you are aiming to use the TensorFlow interface in your personal CMSSW plugin , make sure to include <use name= \"PhysicsTools/TensorFlow\" /> in your plugins/BuildFile.xml . If you are working on a file in the using the interface in a file in the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml next to your src/ or interface/ directories containing (at least): <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export> Single-threaded inference \u00b6 Todo. Full example Todo. Multi-threaded inference \u00b6 Todo. Full example Todo. Links and further reading \u00b6 cmsml package TensorFlow C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession tf.function TensorFlow 2 tutorial Keras API","title":"TensorFlow 2"},{"location":"inference/tensorflow2.html#direct-inference-with-tensorflow-2","text":"TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.2. See the guide on inference with TensorFlow 1 or earlier versions.","title":"Direct inference with TensorFlow 2"},{"location":"inference/tensorflow2.html#software-setup","text":"To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version, you can simply do that via pip # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --install-option = \"--prefix=CUSTOM_DIRECTORY\" cmsml to a location of your choice.","title":"Software setup"},{"location":"inference/tensorflow2.html#saving-your-model","text":"After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) tf.function Let's consider you write your network model in a single tf.function : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"output\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type of shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This attaches a graph object to model but disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph .","title":"Saving your model"},{"location":"inference/tensorflow2.html#inference-in-cmssw","text":"","title":"Inference in CMSSW"},{"location":"inference/tensorflow2.html#cmssw-module-setup","text":"If you are aiming to use the TensorFlow interface in your personal CMSSW plugin , make sure to include <use name= \"PhysicsTools/TensorFlow\" /> in your plugins/BuildFile.xml . If you are working on a file in the using the interface in a file in the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml next to your src/ or interface/ directories containing (at least): <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export>","title":"CMSSW module setup"},{"location":"inference/tensorflow2.html#single-threaded-inference","text":"Todo. Full example Todo.","title":"Single-threaded inference"},{"location":"inference/tensorflow2.html#multi-threaded-inference","text":"Todo. Full example Todo.","title":"Multi-threaded inference"},{"location":"inference/tensorflow2.html#links-and-further-reading","text":"cmsml package TensorFlow C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession tf.function TensorFlow 2 tutorial Keras API","title":"Links and further reading"},{"location":"inference/xgboost.html","text":"Direct inference with XGBoost \u00b6 Todo.","title":"XGBoost"},{"location":"inference/xgboost.html#direct-inference-with-xgboost","text":"Todo.","title":"Direct inference with XGBoost"},{"location":"optimization/introduction.html","text":"Model Optimization \u00b6 Todo.","title":"Model Optimization"},{"location":"optimization/introduction.html#model-optimization","text":"Todo.","title":"Model Optimization"},{"location":"starter/introduction.html","text":"Starter Guide Introdction \u00b6 Todo.","title":"Introduction"},{"location":"starter/introduction.html#starter-guide-introdction","text":"Todo.","title":"Starter Guide Introdction"}]}