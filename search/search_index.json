{"config":{"lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Welcome to the documentation hub for the CMS Machine Learning Group! The goal of this page is to provide CMS analyzers a centralized place to gather machine learning information relevant to their work. However, we are not seeking to rewrite external documentation. Whenever applicable, we will link to external documentation, such as the iML groups HEP Living Review or their ML Resources repository. What you will find here are pages covering: ML best practices How to optimize a NN Common pitfalls for CMS analyzers Direct and indirect inferencing using a variety of ML packages How to get a model integrated into CMSSW And much more! If you think we are missing some important information, please contact the ML Knowledge Subgroup !","title":"Home"},{"location":"inference/checklist.html","text":"Integration checklist \u00b6 Todo.","title":"Integration checklist"},{"location":"inference/checklist.html#integration-checklist","text":"Todo.","title":"Integration checklist"},{"location":"inference/hls4ml.html","text":"Direct inference with hls4ml \u00b6 This page is still under construction. For now, please see the main hls4ml documentation .","title":"hls4ml"},{"location":"inference/hls4ml.html#direct-inference-with-hls4ml","text":"This page is still under construction. For now, please see the main hls4ml documentation .","title":"Direct inference with hls4ml"},{"location":"inference/onnx.html","text":"Direct inference with ONNX \u00b6 Todo.","title":"ONNX"},{"location":"inference/onnx.html#direct-inference-with-onnx","text":"Todo.","title":"Direct inference with ONNX"},{"location":"inference/particlenet.html","text":"ParticleNet \u00b6 ParticleNet [ arXiv:1902.08570 ] is an advanced neural network architecture that has many applications in CMS, including heavy flavour jet tagging, jet mass regression, etc. The network is fed by various low-level point-like objects as input, e.g., the particle-flow candidates, to predict a feature of a jet. The full architecture of the ParticleNet model. We'll walk through the details in the following sections. On this page, we introduce several user-specific aspects of the ParticleNet model. We cover the following items in four sections: An introduction to ParticleNet , including a general description of ParticleNet, the advantages brought from the architecture by concept, a sketch of ParticleNet applications in CMS and other relevant works. An introduction to Weaver and model implementations , introduced in a step-by-step manner: building three network models and understand them from the technical side; using the out-of-the-box commands to run these examples on a benchmark task. The three networks are (1) a simple feed-forward NN, (2) a DeepAK8 model (based on 1D CNN), and eventually (3) the ParticleNet model (based on DGCNN). making the comparison plots. This section is friendly to the ML newcomers. The goal is to help readers understand the underlying structure of the \"ParticleNet\". Tuning the ParticleNet model , including tips for readers who are using/modifying the ParticleNet model to achieve a better performance This section can be helpful in practice. It provides tips on model training, tunning, validation, etc. It targets the situations when readers apply their own ParticleNet (or ParticleNet-like) model to the custom task. Corresponding persons: Huilin Qu, Loukas Gouskos (original developers of ParticleNet) Congqiao Li (author of the page) Introduction to ParticleNet \u00b6 1. General discription \u00b6 ParticleNet is a graph neural net (GNN) model. The key ingredient of ParticleNet is the graph convolutional operation, i.e., the edge convolution (EdgeConv) and the dynamic graph CNN (DGCNN) method [ arXiv:1801.07829 ] applied on the \"point cloud\" data structure. We will disassemble the ParticleNet model and provide a detailed exploration in the next section, but here we briefly explain the key features of the model. Intuitively, ParticleNet treats all candidates inside an object as a \"point cloud\", which is a permutational-invariant set of points (e.g. a set of PF candidates), each carrying a feature vector ( \u03b7 , \u03c6 , p T , charge, etc.). The DGCNN uses the EdgeConv operation to exploit their spatial correlations (two-dimensional on the \u03b7 - \u03c6 plain) by finding the k -nearest neighbours of each point and generate a new latent graph layer where points are scattered on a high-dimensional latent space. This is a graph-type analogue of the classical 2D convolution operation, which acts on a regular 2D grid (e.g., a picture) using a 3\u00d73 local patch to explore the relations of a single-pixel with its 8 nearest pixels, then generates a new 2D grid. The cartoon illustrates the convolutional operation acted on the regular grid and on the point cloud (plot from ML4Jets 2018 talk). As a consequence, the EdgeConv operation transforms the graph to a new graph, which has a changed spatial relationship among points. It then acts on the second graph to produce the third graph, showing the stackability of the convolution operation. This illustrates the \"dynamic\" property as the graph topology changes after each EdgeConv layer. 2. Advantage \u00b6 By concept, the advantage of the network may come from exploiting the permutational-invariant symmetry of the points, which is intrinsic to our physics objects. This symmetry is held naturally in a point cloud representation. In a recent study on jet physics or event-based analysis using ML techniques, there are increasing interest to explore the point cloud data structure. We explain here conceptually why a \"point cloud\" representation outperforms the classical ones, including the variable-length 2D vector structure passing to a 1D CNN or any type of RNN, and imaged-based representation passing through a 2D CNN. By using the 1D CNN, the points (PF candidates) are more often ordered by p T to fix on the 1D grid. Only correlations with neighbouring points with similar p T are learned by the network with a convolution operation. The Long Short-Term Memory (LSTM) type recurrent neural network (RNN) provides the flexibility to feed in a variant-length sequence and has a \"memory\" mechanism to cooperate the information it learns from an early node to the latest node. The concern is that such ordering of the sequence is somewhat artificial, and not an underlying property that an NN must learn to accomplish the classification task. As a comparison, in the task of the natural language processing where LSTM has a huge advantage, the order of words are important characteristic of a language itself (reflects the \"grammar\" in some circumstances) and is a feature the NN must learn to master the language. The imaged-based data explored by a 2D CNN stems from the image recognition task. A jet image with proper standardization is usually performed before feeding into the network. In this sense, it lacks local features which the 2D local patch is better at capturing, e.g. the ear of the cat that a local patch can capture by scanning over the entire image. The jet image is appearing to hold the features globally (e.g. two-prong structure for W-tagging). The sparsity of data is another concern in that it introduces redundant information to present a jet on the regular grid, making the network hard to capture the key properties. 3. Applications and other related work \u00b6 Here we briefly summarize the applications and ongoing works on ParticleNet. Public CMS results include large- R jet with R =0.8 tagging (for W/Z/H/t) using ParticleNet [ CMS-DP-2020/002 ] regression on the large- R jet mass based on the ParticleNet model [ CMS-DP-2020/002 ] ParticleNet architecture is also applied on small radius R =0.4 jets for the b/c-tagging and quark/gluon classification (see this talk (CMS internal) ). A recent ongoing work applies the ParticleNet architecture in heavy flavour tagging at HLT (see this talk (CMS internal) ). The ParticleNet model is recently updated to ParticleNeXt and see further improvement (see the ML4Jets 2021 talk ). Recent works in the joint field of HEP and ML also shed light on exploiting the point cloud data structure and GNN-based architectures. We see very active progress in recent years. Here list some useful materials for the reader's reference. Some pheno-based work are summarized in the HEP \u00d7 ML living review , especially in the \"graph\" and \"sets\" categories. An overview of GNN applications to CMS [ CMS ML forum (CMS internal) ] At the time of writing, various novel GNN-based models are explored and introduced in the recent ML4Jets2021 meeting. Introduction to Weaver and model implementations \u00b6 Weaver is a machine learning R&D framework for high energy physics (HEP) applications. It trains the neural net with PyTorch and is capable of exporting the model to the ONNX format for fast inference. A detailed guide is presented on Weaver README page. Now we walk through three solid examples to get you familiar with Weaver . We use the benchmark of the top tagging task [ arXiv:1707.08966 ] in the following example. Some useful information can be found in the \"top tagging\" section in the IML public datasets webpage (the gDoc ). Our goal is to do some warm-up with Weaver , and more importantly, to explore from a technical side the neural net architectures, from a simple multi-layer perceptron (MLP) model, to a more complicated \"DeepAK8 tagger\" model based on 1D CNN with ResNet, and eventually to the \"ParticleNet model\" which is based on DGCNN. We will dig deeper into their implementations in Weaver and try to illustrate as many details as possible. Finally, we compare their performance and see if we can reproduce the benchmark record with the model. Please clone the repo weaver-benchmark and we'll get started. 1. Build models in Weaver \u00b6 When implementing a new training in Weaver , two key elements are crucial: the model and the data configuration file. The model configuration file includes a get_model function that returns a torch.nn.Module type model and a dictionary of model info used to export an ONNX-format model. The data configuration is a YAML file describing how to process the input data. Please see the Weaver README for details. Before moving on, we need a preprocessing of the benchmark datasets. The original sample is an H5 file including branches like energy E_i and 3-momenta PX_i , PY_i , PZ_i for each jet constituent i ( i =0, ..., 199) inside a jet. All branches are in the 1D flat structure. We reconstruct the data in a way that the jet features are 2D vectors (e.g., in the vector<float> format): E , PX , PY , PZ , with variable-length that corresponds to the number of constituents. Note that this is a commonly used data structure, similar to the NanoAOD format in CMS. The input files after preprocessing (in the .awkd format) can be found at CERN EOS space /eos/user/c/coli/public/weaver-benchmark/samples/top/ . It includes three sets of data for training, validation, and test. Note To preprocess the input files from the original datasets manually, direct to the weaver-benchmark base directory and run python utils / convert_top_datasets . py - i < your - sample - dir > This will convert the .h5 file to the .awkd file and create some new variables for each jet, including the relative \u03b7 and \u03c6 value w.r.t. main axis of the jet of each jet constituent. Then, we show three NN model configurations below and provide detailed explanations of the code. We make meticulous efforts on the illustration of the model architecture, especially in the ParticleNet case. A simple MLP A simple multi-layer perception model is first provided here as proof of the concept. All layers are based on the linear transformation of the 1D vectors. The model configuration card is shown in networks/top/mlp_pf.py . First, we implement an MLP network in the nn.Module class. MLP implementation Also, see networks/top/mlp_pf.py . We elaborate here on several aspects. A sequence of linear layers and ReLU activation functions is defined in nn.Sequential(nn.Linear(channels[i], channels[i + 1]), nn.ReLU() . By combining multiple of them, we construct a simple multi-layer perceptron. The input data x takes the 3D format, in the dimension (N, C, P) , which is decided by our data structure and the data configuration card. Here, N is the mini-batch size, C is the feature size, and P is the size of constituents per jet. To feed into our MLP, we flatten the last two dimensions by x = x.flatten(start_dim=1) to form the vector of dimension (N, L) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiLayerPerceptron ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions. num_classes : int Number of output classes. layer_params : list List of the feature size for each layer. \"\"\" def __init__ ( self , input_dims , num_classes , layer_params = (), ** kwargs ): super ( MultiLayerPerceptron , self ) . __init__ ( ** kwargs ) channels = [ input_dims ] + list ( layer_params ) + [ num_classes ] layers = [] for i in range ( len ( channels ) - 1 ): layers . append ( nn . Sequential ( nn . Linear ( channels [ i ], channels [ i + 1 ]), nn . ReLU ())) self . mlp = nn . Sequential ( * layers ) def forward ( self , x ): # x: the feature vector initally read from the data structure, in dimension (N, C, P) x = x . flatten ( start_dim = 1 ) # (N, L), where L = C * P return self . mlp ( x ) Then, we write the get_model and get_loss functions which will be sent into Weaver 's training code. get_model and get_loss function Also see networks/top/mlp_pf.py . We elaborate here on several aspects. Inside get_model , the model is essentially the MLP class we define, and the model_info takes the default definition, including the input/output shape, the dimensions of the dynamic axes for the input/output data shape that will guide the ONNX model exportation. The get_loss function is not changed as in the classification task we always use the cross-entropy loss function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_model ( data_config , ** kwargs ): layer_params = ( 2048 , 256 , 256 ) _ , pf_length , pf_features_dims = data_config . input_shapes [ 'pf_features' ] input_dims = pf_length * pf_features_dims num_classes = len ( data_config . label_value ) model = MultiLayerPerceptron ( input_dims , num_classes , layer_params = layer_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () Below show the full structure of the MLP network printed by PyTorch. You will see it in the Weaver output during the training. The full-scale structure of the MLP network MultiLayerPerceptron( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (mlp): Sequential( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (0): Sequential( |0.411 M, 55.540% Params, 0.0 GMac, 55.563% MACs| (0): Linear(in_features=400, out_features=1024, bias=True, |0.411 M, 55.540% Params, 0.0 GMac, 55.425% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.138% MACs|) ) (1): Sequential( |0.262 M, 35.492% Params, 0.0 GMac, 35.452% MACs| (0): Linear(in_features=1024, out_features=256, bias=True, |0.262 M, 35.492% Params, 0.0 GMac, 35.418% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (2): Sequential( |0.066 M, 8.899% Params, 0.0 GMac, 8.915% MACs| (0): Linear(in_features=256, out_features=256, bias=True, |0.066 M, 8.899% Params, 0.0 GMac, 8.880% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (3): Sequential( |0.001 M, 0.070% Params, 0.0 GMac, 0.070% MACs| (0): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.070% Params, 0.0 GMac, 0.069% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in data/top/pf_features.yaml . It defines one input group, pf_features , which takes four variables Etarel , Phirel , E_log , P_log . This is based on our data structure, where these variables are 2D vectors with variable lengths. The length is chosen as 100 in a way that the last dimension (the jet constituent dimension) is always truncated or padded to have length 100. In the following two models (i,e., the DeepAK8 and the ParticleNet model) you will see that the data cards are very similar. The change will only be the way we present the input group(s). DeepAK8 (1D CNN) Note The DeepAK8 tagger is a widely used highly-boosted jet tagging in the CMS community. The design of the model can be found in the CMS paper [ arXiv:2004.08262 ]. The original model is trained on MXNet and its configuration can be found here . We now migrate the model architecture to Weaver and train it on Pytorch. Also, we narrow the multi-class output score to the binary output to adapt our binary classification task (top vs. QCD jet). The model card is given in networks/top/deepak8_pf.py . The DeepAK8 model is inspired by the ResNet architecture. The key ingredient is the ResNet unit constructed by multiple CNN layers with a shortcut connection. First, we define the ResNet unit in the model card. ResNet unit implementation See networks/top/deepak8_pf.py . We elaborate here on several aspects. A ResNet unit is made of two 1D CNNs with batch normalization and ReLU activation function. The shortcut is introduced here by directly adding the input data to the processed data after passing the CNN layers. The shortcut connection help to ease the training for the \"deeper\" model [ arXiv:1512.03385 ]. Note that a trivial linear transformation is applied ( self.conv_sc ) if the feature dimension of the input and output data does not match. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class ResNetUnit ( nn . Module ): def __init__ ( self , in_channels , out_channels , strides = ( 1 , 1 ), ** kwargs ): super ( ResNetUnit , self ) . __init__ ( ** kwargs ) self . conv1 = nn . Conv1d ( in_channels , out_channels , kernel_size = 3 , stride = strides [ 0 ], padding = 1 ) self . bn1 = nn . BatchNorm1d ( out_channels ) self . conv2 = nn . Conv1d ( out_channels , out_channels , kernel_size = 3 , stride = strides [ 1 ], padding = 1 ) self . bn2 = nn . BatchNorm1d ( out_channels ) self . relu = nn . ReLU () self . dim_match = True if not in_channels == out_channels or not strides == ( 1 , 1 ): # dimensions not match self . dim_match = False self . conv_sc = nn . Conv1d ( in_channels , out_channels , kernel_size = 1 , stride = strides [ 0 ] * strides [ 1 ], bias = False ) def forward ( self , x ): identity = x x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . conv2 ( x ) x = self . bn2 ( x ) x = self . relu ( x ) # print('resnet unit', identity.shape, x.shape, self.dim_match) if self . dim_match : return identity + x else : return self . conv_sc ( identity ) + x With the ResNet unit, we construct the DeepAK8 model. The model hyperparameters are chosen as follows. conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] DeepAK8 model implementation See networks/top/deepak8_pf.py . Note that the main architecture is a Pytorch re-implementation of the code here based on the MXNet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class ResNet ( nn . Module ): r \"\"\"Parameters ---------- features_dims : int Input feature dimensions. num_classes : int Number of output classes. conv_params : list List of the convolution layer parameters. The first element is a tuple of size 1, defining the transformed feature size for the initial feature convolution layer. The following are tuples of feature size for multiple stages of the ResNet units. Each number defines an individual ResNet unit. \"\"\" def __init__ ( self , features_dims , num_classes , conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )], fc_params = [( 512 , 0.2 )], ** kwargs ): super ( ResNet , self ) . __init__ ( ** kwargs ) self . conv_params = conv_params self . num_stages = len ( conv_params ) - 1 self . fts_conv = nn . Sequential ( nn . Conv1d ( in_channels = features_dims , out_channels = conv_params [ 0 ][ 0 ], kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm1d ( conv_params [ 0 ][ 0 ]), nn . ReLU ()) # define ResNet units for each stage. Each unit is composed of a sequence of ResNetUnit block self . resnet_units = nn . ModuleDict () for i in range ( self . num_stages ): # stack units[i] layers in this stage unit_layers = [] for j in range ( len ( conv_params [ i + 1 ])): in_channels , out_channels = ( conv_params [ i ][ - 1 ], conv_params [ i + 1 ][ 0 ]) if j == 0 \\ else ( conv_params [ i + 1 ][ j - 1 ], conv_params [ i + 1 ][ j ]) strides = ( 2 , 1 ) if ( j == 0 and i > 0 ) else ( 1 , 1 ) unit_layers . append ( ResNetUnit ( in_channels , out_channels , strides )) self . resnet_units . add_module ( 'resnet_unit_ %d ' % i , nn . Sequential ( * unit_layers )) # define fully connected layers fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param in_chn = conv_params [ - 1 ][ - 1 ] if idx == 0 else fc_params [ idx - 1 ][ 0 ] fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) def forward ( self , x ): # x: the feature vector, (N, C, P) x = self . fts_conv ( x ) for i in range ( self . num_stages ): x = self . resnet_units [ 'resnet_unit_ %d ' % i ]( x ) # (N, C', P'), P'<P due to kernal_size>1 or stride>1 # global average pooling x = x . sum ( dim =- 1 ) / x . shape [ - 1 ] # (N, C') # fully connected x = self . fc ( x ) # (N, out_chn) return x def get_model ( data_config , ** kwargs ): conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ResNet ( pf_features_dims , num_classes , conv_params = conv_params , fc_params = fc_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () Below show the full structure of the DeepAK8 model based on 1D CNN with ResNet. It is printed by PyTorch and you will see it in the Weaver output during training. The full-scale structure of the DeepAK8 architecture ResNet( |0.349 M, 100.000% Params, 0.012 GMac, 100.000% MACs| (fts_conv): Sequential( |0.0 M, 0.137% Params, 0.0 GMac, 0.427% MACs| (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(1,), |0.0 M, 0.119% Params, 0.0 GMac, 0.347% MACs|) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.018% Params, 0.0 GMac, 0.053% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs|) ) (resnet_units): ModuleDict( |0.282 M, 80.652% Params, 0.012 GMac, 99.010% MACs| (resnet_unit_0): Sequential( |0.046 M, 13.124% Params, 0.005 GMac, 38.409% MACs| (0): ResNetUnit( |0.021 M, 5.976% Params, 0.002 GMac, 17.497% MACs| (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.006 M, 1.778% Params, 0.001 GMac, 5.175% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) (conv_sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.587% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.003 GMac, 20.912% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) ) ) (resnet_unit_1): Sequential( |0.054 M, 15.471% Params, 0.003 GMac, 22.619% MACs| (0): ResNetUnit( |0.029 M, 8.322% Params, 0.001 GMac, 12.163% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False, |0.004 M, 1.173% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.001 GMac, 10.456% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) (resnet_unit_2): Sequential( |0.182 M, 52.057% Params, 0.005 GMac, 37.982% MACs| (0): ResNetUnit( |0.083 M, 23.682% Params, 0.002 GMac, 17.284% MACs| (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), |0.025 M, 7.075% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False, |0.008 M, 2.346% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.099 M, 28.375% Params, 0.002 GMac, 20.698% MACs| (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) ) (fc): Sequential( |0.067 M, 19.210% Params, 0.0 GMac, 0.563% MACs| (0): Sequential( |0.066 M, 18.917% Params, 0.0 GMac, 0.555% MACs| (0): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 18.917% Params, 0.0 GMac, 0.551% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs|) (2): Dropout(p=0.2, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=512, out_features=2, bias=True, |0.001 M, 0.294% Params, 0.0 GMac, 0.009% MACs|) ) ) The data card is the same as the MLP case, shown in data/top/pf_features.yaml . ParticleNet (DGCNN) Note The ParticleNet model applied to the CMS analysis is provided in networks/particlenet_pf_sv.py , and the data card in data/ak15_points_pf_sv.yaml . Here we use a similar configuration card to deal with the benchmark task. We will elaborate on the ParticleNet model and focus more on the technical side in this section. The model is defined in networks/top/particlenet_pf.py , but it imports some constructor, the EdgeConv block, in weaver/utils/nn/model/ParticleNet.py . The EdgeConv is illustrated in the cartoon. Illustration of the EdgeConv block From an EdgeConv block's point of view, it requires two classes of features as input: the \"coordinates\" and the \"features\". These features are the per point properties, in the 2D shape with dimensions (C, P) , where C is the size of the features (the feature size of \"coordinates\" and the \"features\" can be different, marked as C_pts , C_fts in the following code), and P is the number of points. The block outputs the new features that the model learns, also in the 2D shape with dimensions (C_fts_out, P) . What happens inside the EdgeConv block? And how is the output feature vector transferred from the input features using the topology of the point cloud? The answer is encoded in the edge convolution (EdgeConv). The edge convolution is an analogues convolution method defined on a point cloud, whose shape is given by the \"coordinates\" of points. Specifically, the input \"coordinates\" provide a view of spatial relations of the points in the Euclidean space. It determines the k -nearest neighbouring points for each point that will guide the update of the feature vector of a point. For each point, the updated feature vector is based on the current state of the point and its k neighbours. Guided by this spirit, all features of the point cloud forms a 3D vector with dimensions (C, P, K) , where C is the per-point feature size (e.g., \u03b7 , \u03c6 , p T \uff0c...), P is the number of points, and K the k -NN number. The structured vector is linearly transformed by acting 2D CNN on the feature dimension C . This helps to aggregate the feature information and exploit the correlations of each point with its adjacent points. A shortcut connection is also introduced inspired by the ResNet. Below shows how the EdgeConv structure is implemented in the code. EdgeConv block implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several aspects. The EdgeConvBlock takes the feature dimension in_feat , out_feats which are C_fts , C_fts_out we introduced above. The input data vectors to forward() are \"coordinates\" and \"features\" vector, in the dimension of (N, C_pts(C_fts), P) as introduced above. The first dimension is the mini-batch size. self.get_graph_feature() helps to aggregate k -nearest neighbours for each point. The resulting vector is in the dimension of (N, C_fts, P, K) as we discussed above, K being the k -NN number. After convolutions, the per-point features are merged by taking the mean of all k -nearest neighbouring vectors: fts = x . mean ( dim =- 1 ) # (N, C, P) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class EdgeConvBlock ( nn . Module ): r \"\"\"EdgeConv layer. Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds <https://arxiv.org/pdf/1801.07829>`__\". Can be described as follows: .. math:: x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}( \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)}) where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. batch_norm : bool Whether to include batch normalization on messages. \"\"\" def __init__ ( self , k , in_feat , out_feats , batch_norm = True , activation = True , cpu_mode = False ): super ( EdgeConvBlock , self ) . __init__ () self . k = k self . batch_norm = batch_norm self . activation = activation self . num_layers = len ( out_feats ) self . get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1 self . convs = nn . ModuleList () for i in range ( self . num_layers ): self . convs . append ( nn . Conv2d ( 2 * in_feat if i == 0 else out_feats [ i - 1 ], out_feats [ i ], kernel_size = 1 , bias = False if self . batch_norm else True )) if batch_norm : self . bns = nn . ModuleList () for i in range ( self . num_layers ): self . bns . append ( nn . BatchNorm2d ( out_feats [ i ])) if activation : self . acts = nn . ModuleList () for i in range ( self . num_layers ): self . acts . append ( nn . ReLU ()) if in_feat == out_feats [ - 1 ]: self . sc = None else : self . sc = nn . Conv1d ( in_feat , out_feats [ - 1 ], kernel_size = 1 , bias = False ) self . sc_bn = nn . BatchNorm1d ( out_feats [ - 1 ]) if activation : self . sc_act = nn . ReLU () def forward ( self , points , features ): # points: (N, C_pts, P) # features: (N, C_fts, P) # N: batch size, C: feature size per point, P: number of points topk_indices = knn ( points , self . k ) # (N, P, K) x = self . get_graph_feature ( features , self . k , topk_indices ) # (N, C_fts, P, K) for conv , bn , act in zip ( self . convs , self . bns , self . acts ): x = conv ( x ) # (N, C', P, K) if bn : x = bn ( x ) if act : x = act ( x ) fts = x . mean ( dim =- 1 ) # (N, C, P) # shortcut if self . sc : sc = self . sc ( features ) # (N, C_out, P) sc = self . sc_bn ( sc ) else : sc = features return self . sc_act ( sc + fts ) # (N, C_out, P) With the EdgeConv architecture as the building block, the ParticleNet model is constructed as follow. The ParticleNet model stacks three EdgeConv blocks to construct higher-level features and passing them through the pipeline. The points (i.e., in our case, the particle candidates inside a jet) are not changing, but the per-point \"coordinates\" and \"features\" vectors changes, in both values and dimensions. For the first EdgeConv block, the \"coordinates\" only includes the relative \u03b7 and \u03c6 value of each particle. The \"features\" is a vector with a standard length of 32, which is linearly transformed from the initial feature vectors including the components of relative \u03b7 , \u03c6 , the log of p T , etc. The first EdgeConv block outputs a per-point feature vector of length 64, which is taken as both the \"coordinates\" and \"features\" to the next EdgeConv block. That is to say, the next k -NN is applied on the 64D high-dimensional spatial space to capture the new relations of points learned by the model. This is visualized by the input/output array showing the data flow of the model. We see that this architecture illustrates the stackability of the EdgeConv block, and is the core to the Dynamic Graph CNN (DGCNN), as the model can dynamically change the correlations of each point based on learnable features. A fusion technique is also used by concatenating the three EdgeConv output vectors together (adding the dimensions), instead of using the last EdgeConv output, to form an output vector. This is also one form of shortcut implementations that helps to ease the training for a complex and deep convolutional network model. The concatenated vectors per point are then averaged over points to produce a single 1D vector of the whole point cloud. The vector passes through one fully connected layer, with a dropout rate of p=0.1 to prevent overfitting. Then, in our example, the full network outputs two scores after a softmax, representing the one-hot encoding of the top vs. QCD class. The ParticleNet implementation is shown below. ParticleNet model implementation See utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several mean points. The stack of multiple EdgeConv blocks are implemented in for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask The multiple EdgeConv layer parameters are given by conv_params , which takes a list of tuples, each tuple in the format of (K, (C1, C2, C3)) . K for the k -NN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. The fully connected layer parameters are given by fc_params , which takes a list of tuples, each tuple in the format of (n_feat, drop_rate) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class ParticleNet ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions (C_fts). num_classes : int Number of output classes. conv_params : list List of convolution parameters of EdgeConv blocks, each element in the format of (K, (C1, C2, C3)). K for the kNN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) use_fusion: bool If true, concatenates all output features from each EdgeConv before the fully connected layer. use_fts_bn: bool If true, applies a batch norm before feeding to the EdgeConv block. use_counts: bool If true, uses the real count of points instead of the padded size (the max point size). for_inference: bool Whether this is an inference routine. If true, applies a softmax to the output. for_segmentation: bool Whether the model is set up for the point cloud segmentation (instead of classification) task. If true, does not merge the features after the last EdgeConv, and apply Conv1D instead of the linear layer. The output is hence each output_features per point, instead of output_features. \"\"\" def __init__ ( self , input_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , for_inference = False , for_segmentation = False , ** kwargs ): super ( ParticleNet , self ) . __init__ ( ** kwargs ) self . use_fts_bn = use_fts_bn if self . use_fts_bn : self . bn_fts = nn . BatchNorm1d ( input_dims ) self . use_counts = use_counts self . edge_convs = nn . ModuleList () for idx , layer_param in enumerate ( conv_params ): k , channels = layer_param in_feat = input_dims if idx == 0 else conv_params [ idx - 1 ][ 1 ][ - 1 ] self . edge_convs . append ( EdgeConvBlock ( k = k , in_feat = in_feat , out_feats = channels , cpu_mode = for_inference )) self . use_fusion = use_fusion if self . use_fusion : in_chn = sum ( x [ - 1 ] for _ , x in conv_params ) out_chn = np . clip (( in_chn // 128 ) * 128 , 128 , 1024 ) self . fusion_block = nn . Sequential ( nn . Conv1d ( in_chn , out_chn , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( out_chn ), nn . ReLU ()) self . for_segmentation = for_segmentation fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param if idx == 0 : in_chn = out_chn if self . use_fusion else conv_params [ - 1 ][ 1 ][ - 1 ] else : in_chn = fc_params [ idx - 1 ][ 0 ] if self . for_segmentation : fcs . append ( nn . Sequential ( nn . Conv1d ( in_chn , channels , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) else : fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) if self . for_segmentation : fcs . append ( nn . Conv1d ( fc_params [ - 1 ][ 0 ], num_classes , kernel_size = 1 )) else : fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) self . for_inference = for_inference def forward ( self , points , features , mask = None ): # print('points:\\n', points) # print('features:\\n', features) if mask is None : mask = ( features . abs () . sum ( dim = 1 , keepdim = True ) != 0 ) # (N, 1, P) points *= mask features *= mask coord_shift = ( mask == 0 ) * 1e9 if self . use_counts : counts = mask . float () . sum ( dim =- 1 ) counts = torch . max ( counts , torch . ones_like ( counts )) # >=1 if self . use_fts_bn : fts = self . bn_fts ( features ) * mask else : fts = features outputs = [] for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask if self . use_fusion : outputs . append ( fts ) if self . use_fusion : fts = self . fusion_block ( torch . cat ( outputs , dim = 1 )) * mask # assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0) if self . for_segmentation : x = fts else : if self . use_counts : x = fts . sum ( dim =- 1 ) / counts # divide by the real counts else : x = fts . mean ( dim =- 1 ) output = self . fc ( x ) if self . for_inference : output = torch . softmax ( output , dim = 1 ) # print('output:\\n', output) return output Above are the capsulation of all ParticleNet building blocks. Eventually, we have the model defined in the model card networks/benchmark/particlenet.py , in the ParticleNetTagger1Path class, meaning we only use the ParticleNet pipeline that deals with one set of the point cloud (i.e., the particle candidates). Info Two sets of point clouds in the CMS application, namely the particle-flow candidates and secondary vertices, are used. This requires special handling to merge the clouds before feeding them to the first layer of EdgeConv. ParticleNet model config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 import torch import torch.nn as nn from utils.nn.model.ParticleNet import ParticleNet , FeatureConv class ParticleNetTagger1Path ( nn . Module ): def __init__ ( self , pf_features_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , pf_input_dropout = None , for_inference = False , ** kwargs ): super ( ParticleNetTagger1Path , self ) . __init__ ( ** kwargs ) self . pf_input_dropout = nn . Dropout ( pf_input_dropout ) if pf_input_dropout else None self . pf_conv = FeatureConv ( pf_features_dims , 32 ) self . pn = ParticleNet ( input_dims = 32 , num_classes = num_classes , conv_params = conv_params , fc_params = fc_params , use_fusion = use_fusion , use_fts_bn = use_fts_bn , use_counts = use_counts , for_inference = for_inference ) def forward ( self , pf_points , pf_features , pf_mask ): if self . pf_input_dropout : pf_mask = ( self . pf_input_dropout ( pf_mask ) != 0 ) . float () pf_points *= pf_mask pf_features *= pf_mask return self . pn ( pf_points , self . pf_conv ( pf_features * pf_mask ) * pf_mask , pf_mask ) def get_model ( data_config , ** kwargs ): conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] use_fusion = True pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ParticleNetTagger1Path ( pf_features_dims , num_classes , conv_params , fc_params , use_fusion = use_fusion , use_fts_bn = kwargs . get ( 'use_fts_bn' , False ), use_counts = kwargs . get ( 'use_counts' , True ), pf_input_dropout = kwargs . get ( 'pf_input_dropout' , None ), for_inference = kwargs . get ( 'for_inference' , False ) ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The most important parameters are conv_params and fc_params , which decides the model parameters of EdgeConv blocks and the fully connected layer. See details in the above \"ParticleNet model implementation\" box. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] A full structure printed from PyTorch is shown below. It will appear in the Weaver output during training. ParticleNet full-scale structure ParticleNetTagger1Path( |0.577 M, 100.000% Params, 0.441 GMac, 100.000% MACs| (pf_conv): FeatureConv( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (conv): Sequential( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.001% Params, 0.0 GMac, 0.000% MACs|) (1): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.011% Params, 0.0 GMac, 0.001% MACs|) (3): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) ) (pn): ParticleNet( |0.577 M, 99.965% Params, 0.441 GMac, 99.995% MACs| (edge_convs): ModuleList( |0.305 M, 52.823% Params, 0.424 GMac, 96.047% MACs| (0): EdgeConvBlock( |0.015 M, 2.575% Params, 0.021 GMac, 4.716% MACs| (convs): ModuleList( |0.012 M, 2.131% Params, 0.02 GMac, 4.456% MACs| (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) ) (bns): ModuleList( |0.0 M, 0.067% Params, 0.001 GMac, 0.139% MACs| (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.0 GMac, 0.070% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) ) (sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.355% Params, 0.0 GMac, 0.046% MACs|) (sc_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) (1): EdgeConvBlock( |0.058 M, 10.121% Params, 0.081 GMac, 18.437% MACs| (convs): ModuleList( |0.049 M, 8.523% Params, 0.079 GMac, 17.825% MACs| (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) ) (bns): ModuleList( |0.001 M, 0.133% Params, 0.001 GMac, 0.279% MACs| (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.139% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) ) (sc): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False, |0.008 M, 1.420% Params, 0.001 GMac, 0.186% MACs|) (sc_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.006% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs|) ) (2): EdgeConvBlock( |0.231 M, 40.128% Params, 0.322 GMac, 72.894% MACs| (convs): ModuleList( |0.197 M, 34.091% Params, 0.315 GMac, 71.299% MACs| (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) ) (bns): ModuleList( |0.002 M, 0.266% Params, 0.002 GMac, 0.557% MACs| (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.279% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) ) (sc): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False, |0.033 M, 5.682% Params, 0.003 GMac, 0.743% MACs|) (sc_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.012% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs|) ) ) (fusion_block): Sequential( |0.173 M, 29.963% Params, 0.017 GMac, 3.925% MACs| (0): Conv1d(448, 384, kernel_size=(1,), stride=(1,), bias=False, |0.172 M, 29.830% Params, 0.017 GMac, 3.899% MACs|) (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.133% Params, 0.0 GMac, 0.017% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs|) ) (fc): Sequential( |0.099 M, 17.179% Params, 0.0 GMac, 0.023% MACs| (0): Sequential( |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs| (0): Linear(in_features=384, out_features=256, bias=True, |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) (2): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in data/top/pf_features.yaml , given in a similar way as in the MLP example. Here we group the inputs into three classes: pf_points , pf_features and pf_masks . They correspond to the forward(self, pf_points, pf_features, pf_mask) prototype of our nn.Module model, and will send in these 2D vectors in the mini-batch size for each iteration during training/prediction. ParticleNet data config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression pf_mask : awkward.JaggedArray.ones_like(E) is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_points : length : 100 vars : - [ PX , 0 , 0.05 ] - [ PY , 0 , 0.05 ] - [ PZ , 0 , 0.05 ] pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - [ PX , 0 , 0.05 ] - [ PY , 0 , 0.05 ] - [ PZ , 0 , 0.05 ] - [ E_log , 2 , 1 ] pf_mask : length : 100 vars : - pf_mask labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - E_tot - PX_tot - PY_tot - PZ_tot - P_tot - Eta_tot - Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms Above, we analyze the three networks in a detailed manner. As a summary, we draw the three network architectures in the following three cartoons for a more overall and comprehensive understanding. The full architecture of the proof-of-concept multi-layer perception model. The full architecture of the DeepAK8 model, which is based on 1D CNN with ResNet architecture. The full architecture of the ParticleNet model, which is based on DGCNN and EdgeConv. The model and data configuration cards, the number of parameters, and computational complexity are summarized in the following table. Note that we'll refer to the shell variables provided here in the following training example. Model ${prefix} ${model_config} ${data_config} Parameters Computational complexity MLP mlp ../networks/top/mlp_pf.py ../data/top/pf_features.yml 739k 0.001 GMac DeepAK8 (1D CNN) deepak8 ../networks/top/deepak8.py ../data/top/pf_features.yml 349k 0.012 GMac ParticleNet (DGCNN) particlenet ../networks/top/particlenet_pf.py ../data/top/pf_points_features.yml 577k 0.441 GMac 2. Start training! \u00b6 Now we train the three neural networks based on the provided model and data configurations. Here we present three ways of training. For readers who have a local machine with CUDA GPUs, please try out training on the local GPUs. Readers who would like to try on CPUs can also refer to the local GPU instruction. It is also possible to borrow the GPU resources from the lxplus condor or CMS-connect. Please find in the following that meets your situation. Train on local GPUs The three networks can be trained with a universal script. Enter the weaver base folder and run the following command. Note that ${data_config} , ${model_config} , and ${prefix} refers to the value in the above table for each example, and the fake path should be replaced with the correct one. python train . py \\ -- data - train '<path-to-samples>/prep/top_train_*.awkd' \\ -- data - val '<path-to-samples>/prep/top_val_*.awkd' \\ -- fetch - by - file -- fetch - step 1 \\ -- num - workers 3 \\ -- data - config data / benchmark / $ { data_config } \\ -- network - config networks / benchmark / $ { model_config } \\ -- model - prefix output / $ { prefix } \\ -- gpus 0 , 1 -- batch - size 1024 -- start - lr 5e-3 -- num - epochs 20 -- optimizer ranger \\ -- log output / $ { prefix } . train . log Here are several caveats running this command. Warning --fetch-by-file is very important when you have only a few input files. Please see Weaver README for more details on this argument. A common situation is that when you have only one input file that includes both signal and backgrounds, but events are not well mixed. Then, it is a must to use --fetch-by-file , because otherwise, only a small collection of events on the top are read into the memory and sending to the mini-batch after shuffling. In such cases, events can have a strong bias in different categories (the most extreme case is that it only contains one category) and may confuse the training. By using --fetch-by-file , all events in a file are read into the memory, shuffled properly and sent to the mini-batch. Specify GPUS based on their device IDs. For running on CPUs, please use --gpu '' . Then, predict the score on the test datasets using the best model: python train . py -- predict \\ -- data - test '<path-to-samples>/prep/top_test_*.awkd' \\ -- num - workers 3 \\ -- data - config data / benchmark / $ { data_config } \\ -- network - config networks / benchmark / $ { model_config } \\ -- model - prefix output / $ { prefix } _best_epoch_state . pt \\ -- gpus 0 , 1 -- batch - size 1024 \\ -- predict - output output / $ { prefix } _predict . root Use GPUs on lxplus HTCondor On lxplus HTCondor, the GPU(s) can be booked via the arguments request_gpus . To get familiar with the GPU service, please refer to the documentation here . While it is not possible to test the script locally, you can try out the condor_ssh_to_job command to connect to the remote condor machine that runs the jobs. This interesting feature will help you with debugging or monitoring the condor job. Here we provide the example executed script and the condor submitted file for the training and predicting task. Create the following two files: The executable: run.sh Still, please remember to specify ${data_config} , ${model_config} , and ${prefix} as shown in the above table, and replace the fake path with the correct one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #!/bin/bash WORKDIR = ` pwd ` # Download miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda_install.sh bash miniconda_install.sh -b -p ${ WORKDIR } /miniconda export PATH = $WORKDIR /miniconda/bin: $PATH pip install numpy pandas scikit-learn scipy matplotlib tqdm PyYAML pip install uproot3 awkward0 lz4 xxhash pip install tables pip install onnxruntime-gpu pip install torch # CUDA environment setup export PATH = $PATH :/usr/local/cuda-10.2/bin export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/usr/local/cuda-10.2/lib64 export LIBRARY_PATH = $LIBRARY_PATH :/usr/local/cuda-10.2/lib64 # Clone weaver git clone https://github.com/hqucms/weaver.git cd weaver/ mkdir output # Training python train.py \\ --data-train '<path-to-samples>/prep/top_train_*.awkd' \\ --data-val '<path-to-samples>/prep/top_val_*.awkd' \\ --fetch-by-file --fetch-step 1 \\ --num-workers 3 \\ --data-config data/benchmark/ ${ data_config } \\ --network-config networks/benchmark/ ${ model_config } \\ --model-prefix output/ ${ prefix } \\ --gpus 0 --batch-size 1024 --start-lr 5e-3 --num-epochs 20 --optimizer ranger \\ --log output/ ${ prefix } .train.log # Predicting score python train.py --predict \\ --data-test '<path-to-samples>/prep/top_test_*.awkd' \\ --num-workers 3 \\ --data-config data/benchmark/ ${ data_config } \\ --network-config networks/benchmark/ ${ model_config } \\ --model-prefix output/ ${ prefix } _best_epoch_state.pt \\ --gpus 0 --batch-size 1024 \\ --predict-output output/ ${ prefix } _predict.root HTCondor submitted file: submit.sub 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Universe = vanilla executable = run.sh arguments = output = logs/$(ClusterId).$(ProcId).out error = logs/$(ClusterId).$(ProcId).err log = logs/$(ClusterId).log should_transfer_files = YES when_to_transfer_output = ON_EXIT_OR_EVICT transfer_output_files = weaver/output transfer_output_remaps = \"output = output.$(ClusterId).$(ProcId)\" request_GPUs = 1 request_CPUs = 2 +MaxRuntime = 604800 queue Make the run.sh script an executable, then submit the job. chmod +x run.sh condor_submit submit.sh The weaver/output directory will be transferred back. Use GPUs on CMS Connect CMS Connect provides several GPU nodes. ... 3. Evaluation of models \u00b6 In the output folder, we find the trained PyTorch models after every epoch and the log file that records the loss and accuracy in the runtime. The predict step also produces a predicted root file in the output folder, including the truth label, the predicted store, and several observer variables we provided in the data card. With the predicted root file, we make the ROC curve comparing the performance of the three trained models. Here is the result from my training: Model AUC Accuracy 1/ e B (@ e S =0.3) MLP 0.961 0.898 186 DeepAK8 (1D CNN) 0.979 0.927 585 ParticleNet (DGCNN) 0.984 0.936 1030 We see that the ParticleNet model shows an outstanding performance in this classification task. Besides, the DeepAK8 and ParticleNet results are similar to the benchmark values found in the gDoc . We address that the performance can be further improved by fine-tuning on the model, and by taking the average predicted score from an ensemble of the trained model with different initial parametrization - a well known ML technique to pursue an extra few percent of improvements. Tuning the ParticleNet model \u00b6 When it comes to the real application of any DNN model, tunning the hyperparameters is an important path towards a better performance. In this section, we provide some tips on the ParticleNet model tunning. For a more detailed discussion on this topic, see more in the \"validation\" chapter in the documentation. 1. Choices on the optimizer and the learning rate \u00b6 The optimizer decides how our neural network update all its parameters, and the learning rate means how fast the parameters changes in one training iteration. Learning rate is the most important hyperparameter to choose from before concrete training is done. Here we quote from a suggested strategy: if you only have the opportunity to optimize one hyperparameter, choose the learning rate. The optimizer is also important because a wiser strategy usually means avoid the zig-zagging updating route, avoid falling into the local minima and even adapting different strategies for the fast-changing parameters and the slow ones. Adam (and its several variations) is a widely used optimizer. Another recently developed advanced optimizer is Ranger that combines RAdam and LookAhead. However, one should note that the few percent level improvement by using different optimizers is likely to be smeared by an unoptimized learning rate. The above training scheme uses a start learning rate of 5e-3, and Ranger as the optimizer. It uses a flat+decay schedular, in a way that the LR starts to decay after processing 70% of epochs, and gradually reduce to 0.1 of its original value when nearing the completion of all epochs. First, we note that the current case is already well optimized. Therefore, by simply reuse the current choice, the training will converge to a stable result in general. But it is always good in practice to test several choices of the optimizer and reoptimize the learning rate. Weaver integrates multiple optimizers. In the above training command, we use --optimizer ranger to adopt the Ranger optimizer. It is also possible to switch to --optimizer adam or --optimizer adamW . Weaver also provides the interface to optimize the learning rate before real training is performed. In the ParticleNet model training, we append --lr-finder 5e-4,5e-1,1000 in the command, then a specific learning-rate finder program will be launched. This setup scans over the LR from 5e-4 to 5e-1 by applying 1000 mini-batches of training. It outputs a plot showing the training loss for different starting learning rates. In general, a lower training loss means a better choice of the learning rate parameter. Below shows the results from LR finder by specifying --lr-finder 5e-4,5e-1,200 (upper) and --lr-finder 5e-4,5e-1,1000 (lower), for the --optimizer adamW (left) and the --optimizer ranger (right) case. The plots show that the training loss forms a basin over a wide range of the learning rate. Therefore, the LR finder only provides a rough estimation. But it is a good attempt to first run the LR finder to have an overall feeling. Besides, we should be aware that different optimizer takes different optimal LR values. As can be seen here, the AdamW in general requires a small LR than Ranger. In the choice of Ranger, we further optimize the LR based on the full training of our model. The reason is that although training the first set of 1000 mini-batches cannot manifest their true impact, in the real training when we have a scheduler to adapt smaller LR gradually, the difference of starting LR choices may take effect. To monitor the full training/evaluation accuracy and the loss for each mini-batch, we can draw support from a nicely integrated utility, TensorBoard, to employ real-time monitoring. To activate TensorBoard, append (note that replace ${prefix} according to the above table) --tensorboard ${ prefix } to the training command, then direct to https://localhost:6008 for the TensorBoard UI. The below plots show the training and evaluation loss, in our standard choice with LR being 5e-3, and in the case of a small LR 2e-3 and a large LR 1e-2. Note that all tested LR values are within the basin in the LR finder plots. We see that in the evaluated loss plot, the standard LR outperforms two variational choices. The reason may be that a larger LR finds difficulty in converging to the global minima, while a smaller LR may not be adequate to reach the minima point in a journey of 20 epochs. Overall, we see 5e-3 as a good choice as the starting LR for the Ranger optimizer. 2. Optimize the model \u00b6 In practice, tuning the model size is also an important task. By concept, a smaller model tends to have unsatisfactory performance due to the limited ability to learn many local features. As the model size goes up, the performance will climb to some extent, but may further decrease due to the network \"degradation\" (deeper models have difficulty learning features). Besides, a heavier model may also cause the overfitting issue. In practice, it also leads to larger inference time which is the main concern when coming to real applications. For the ParticleNet model case, we also test between a smaller and larger variation of the model size. Recall that the original model is defined by the following layer parameters. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] We can replace the code block with ec_k = kwargs . get ( 'ec_k' , 16 ) ec_c1 = kwargs . get ( 'ec_c1' , 64 ) ec_c2 = kwargs . get ( 'ec_c2' , 128 ) ec_c3 = kwargs . get ( 'ec_c3' , 256 ) fc_c , fc_p = kwargs . get ( 'fc_c' , 256 ), kwargs . get ( 'fc_p' , 0.1 ) conv_params = [ ( ec_k , ( ec_c1 , ec_c1 , ec_c1 )), ( ec_k , ( ec_c2 , ec_c2 , ec_c2 )), ( ec_k , ( ec_c3 , ec_c3 , ec_c3 )), ] fc_params = [( fc_c , fc_p )] Then we have the ability to tune the model parameters from the command line. Append the extra arguments in the training command --network-option ec_k 32 --network-option ec_c1 128 --network-option ec_c2 192 --network-option ec_c3 256 and the model parameters will take the new values as specified. We test over two cases, one with the above setting to enlarge the model, and another by using --network-option ec_c1 64 --network-option ec_c2 64 --network-option ec_c3 96 to adopt a lite version. The Tensorboard monitoring plots in the training/evaluation loss is shown as follows. We see that the \"heavy\" model reaches even smaller training loss, meaning that the model does not meet the degradation issue yet. However, the evaluation loss is not catching up with the training loss, showing some degree of overtraining in this scheme. From the evaluation result, we see no improvement by moving to a heavy model. Above, we discuss in a very detailed manner on various attempts we can make to optimize the model. We hope the practical experiences presented here will help readers develop and deploy the complex ML model.","title":"ParticleNet"},{"location":"inference/particlenet.html#particlenet","text":"ParticleNet [ arXiv:1902.08570 ] is an advanced neural network architecture that has many applications in CMS, including heavy flavour jet tagging, jet mass regression, etc. The network is fed by various low-level point-like objects as input, e.g., the particle-flow candidates, to predict a feature of a jet. The full architecture of the ParticleNet model. We'll walk through the details in the following sections. On this page, we introduce several user-specific aspects of the ParticleNet model. We cover the following items in four sections: An introduction to ParticleNet , including a general description of ParticleNet, the advantages brought from the architecture by concept, a sketch of ParticleNet applications in CMS and other relevant works. An introduction to Weaver and model implementations , introduced in a step-by-step manner: building three network models and understand them from the technical side; using the out-of-the-box commands to run these examples on a benchmark task. The three networks are (1) a simple feed-forward NN, (2) a DeepAK8 model (based on 1D CNN), and eventually (3) the ParticleNet model (based on DGCNN). making the comparison plots. This section is friendly to the ML newcomers. The goal is to help readers understand the underlying structure of the \"ParticleNet\". Tuning the ParticleNet model , including tips for readers who are using/modifying the ParticleNet model to achieve a better performance This section can be helpful in practice. It provides tips on model training, tunning, validation, etc. It targets the situations when readers apply their own ParticleNet (or ParticleNet-like) model to the custom task. Corresponding persons: Huilin Qu, Loukas Gouskos (original developers of ParticleNet) Congqiao Li (author of the page)","title":"ParticleNet"},{"location":"inference/particlenet.html#introduction-to-particlenet","text":"","title":"Introduction to ParticleNet"},{"location":"inference/particlenet.html#1-general-discription","text":"ParticleNet is a graph neural net (GNN) model. The key ingredient of ParticleNet is the graph convolutional operation, i.e., the edge convolution (EdgeConv) and the dynamic graph CNN (DGCNN) method [ arXiv:1801.07829 ] applied on the \"point cloud\" data structure. We will disassemble the ParticleNet model and provide a detailed exploration in the next section, but here we briefly explain the key features of the model. Intuitively, ParticleNet treats all candidates inside an object as a \"point cloud\", which is a permutational-invariant set of points (e.g. a set of PF candidates), each carrying a feature vector ( \u03b7 , \u03c6 , p T , charge, etc.). The DGCNN uses the EdgeConv operation to exploit their spatial correlations (two-dimensional on the \u03b7 - \u03c6 plain) by finding the k -nearest neighbours of each point and generate a new latent graph layer where points are scattered on a high-dimensional latent space. This is a graph-type analogue of the classical 2D convolution operation, which acts on a regular 2D grid (e.g., a picture) using a 3\u00d73 local patch to explore the relations of a single-pixel with its 8 nearest pixels, then generates a new 2D grid. The cartoon illustrates the convolutional operation acted on the regular grid and on the point cloud (plot from ML4Jets 2018 talk). As a consequence, the EdgeConv operation transforms the graph to a new graph, which has a changed spatial relationship among points. It then acts on the second graph to produce the third graph, showing the stackability of the convolution operation. This illustrates the \"dynamic\" property as the graph topology changes after each EdgeConv layer.","title":"1. General discription"},{"location":"inference/particlenet.html#2-advantage","text":"By concept, the advantage of the network may come from exploiting the permutational-invariant symmetry of the points, which is intrinsic to our physics objects. This symmetry is held naturally in a point cloud representation. In a recent study on jet physics or event-based analysis using ML techniques, there are increasing interest to explore the point cloud data structure. We explain here conceptually why a \"point cloud\" representation outperforms the classical ones, including the variable-length 2D vector structure passing to a 1D CNN or any type of RNN, and imaged-based representation passing through a 2D CNN. By using the 1D CNN, the points (PF candidates) are more often ordered by p T to fix on the 1D grid. Only correlations with neighbouring points with similar p T are learned by the network with a convolution operation. The Long Short-Term Memory (LSTM) type recurrent neural network (RNN) provides the flexibility to feed in a variant-length sequence and has a \"memory\" mechanism to cooperate the information it learns from an early node to the latest node. The concern is that such ordering of the sequence is somewhat artificial, and not an underlying property that an NN must learn to accomplish the classification task. As a comparison, in the task of the natural language processing where LSTM has a huge advantage, the order of words are important characteristic of a language itself (reflects the \"grammar\" in some circumstances) and is a feature the NN must learn to master the language. The imaged-based data explored by a 2D CNN stems from the image recognition task. A jet image with proper standardization is usually performed before feeding into the network. In this sense, it lacks local features which the 2D local patch is better at capturing, e.g. the ear of the cat that a local patch can capture by scanning over the entire image. The jet image is appearing to hold the features globally (e.g. two-prong structure for W-tagging). The sparsity of data is another concern in that it introduces redundant information to present a jet on the regular grid, making the network hard to capture the key properties.","title":"2. Advantage"},{"location":"inference/particlenet.html#3-applications-and-other-related-work","text":"Here we briefly summarize the applications and ongoing works on ParticleNet. Public CMS results include large- R jet with R =0.8 tagging (for W/Z/H/t) using ParticleNet [ CMS-DP-2020/002 ] regression on the large- R jet mass based on the ParticleNet model [ CMS-DP-2020/002 ] ParticleNet architecture is also applied on small radius R =0.4 jets for the b/c-tagging and quark/gluon classification (see this talk (CMS internal) ). A recent ongoing work applies the ParticleNet architecture in heavy flavour tagging at HLT (see this talk (CMS internal) ). The ParticleNet model is recently updated to ParticleNeXt and see further improvement (see the ML4Jets 2021 talk ). Recent works in the joint field of HEP and ML also shed light on exploiting the point cloud data structure and GNN-based architectures. We see very active progress in recent years. Here list some useful materials for the reader's reference. Some pheno-based work are summarized in the HEP \u00d7 ML living review , especially in the \"graph\" and \"sets\" categories. An overview of GNN applications to CMS [ CMS ML forum (CMS internal) ] At the time of writing, various novel GNN-based models are explored and introduced in the recent ML4Jets2021 meeting.","title":"3. Applications and other related work"},{"location":"inference/particlenet.html#introduction-to-weaver-and-model-implementations","text":"Weaver is a machine learning R&D framework for high energy physics (HEP) applications. It trains the neural net with PyTorch and is capable of exporting the model to the ONNX format for fast inference. A detailed guide is presented on Weaver README page. Now we walk through three solid examples to get you familiar with Weaver . We use the benchmark of the top tagging task [ arXiv:1707.08966 ] in the following example. Some useful information can be found in the \"top tagging\" section in the IML public datasets webpage (the gDoc ). Our goal is to do some warm-up with Weaver , and more importantly, to explore from a technical side the neural net architectures, from a simple multi-layer perceptron (MLP) model, to a more complicated \"DeepAK8 tagger\" model based on 1D CNN with ResNet, and eventually to the \"ParticleNet model\" which is based on DGCNN. We will dig deeper into their implementations in Weaver and try to illustrate as many details as possible. Finally, we compare their performance and see if we can reproduce the benchmark record with the model. Please clone the repo weaver-benchmark and we'll get started.","title":"Introduction to Weaver and model implementations"},{"location":"inference/particlenet.html#1-build-models-in-weaver","text":"When implementing a new training in Weaver , two key elements are crucial: the model and the data configuration file. The model configuration file includes a get_model function that returns a torch.nn.Module type model and a dictionary of model info used to export an ONNX-format model. The data configuration is a YAML file describing how to process the input data. Please see the Weaver README for details. Before moving on, we need a preprocessing of the benchmark datasets. The original sample is an H5 file including branches like energy E_i and 3-momenta PX_i , PY_i , PZ_i for each jet constituent i ( i =0, ..., 199) inside a jet. All branches are in the 1D flat structure. We reconstruct the data in a way that the jet features are 2D vectors (e.g., in the vector<float> format): E , PX , PY , PZ , with variable-length that corresponds to the number of constituents. Note that this is a commonly used data structure, similar to the NanoAOD format in CMS. The input files after preprocessing (in the .awkd format) can be found at CERN EOS space /eos/user/c/coli/public/weaver-benchmark/samples/top/ . It includes three sets of data for training, validation, and test. Note To preprocess the input files from the original datasets manually, direct to the weaver-benchmark base directory and run python utils / convert_top_datasets . py - i < your - sample - dir > This will convert the .h5 file to the .awkd file and create some new variables for each jet, including the relative \u03b7 and \u03c6 value w.r.t. main axis of the jet of each jet constituent. Then, we show three NN model configurations below and provide detailed explanations of the code. We make meticulous efforts on the illustration of the model architecture, especially in the ParticleNet case. A simple MLP A simple multi-layer perception model is first provided here as proof of the concept. All layers are based on the linear transformation of the 1D vectors. The model configuration card is shown in networks/top/mlp_pf.py . First, we implement an MLP network in the nn.Module class. MLP implementation Also, see networks/top/mlp_pf.py . We elaborate here on several aspects. A sequence of linear layers and ReLU activation functions is defined in nn.Sequential(nn.Linear(channels[i], channels[i + 1]), nn.ReLU() . By combining multiple of them, we construct a simple multi-layer perceptron. The input data x takes the 3D format, in the dimension (N, C, P) , which is decided by our data structure and the data configuration card. Here, N is the mini-batch size, C is the feature size, and P is the size of constituents per jet. To feed into our MLP, we flatten the last two dimensions by x = x.flatten(start_dim=1) to form the vector of dimension (N, L) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class MultiLayerPerceptron ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions. num_classes : int Number of output classes. layer_params : list List of the feature size for each layer. \"\"\" def __init__ ( self , input_dims , num_classes , layer_params = (), ** kwargs ): super ( MultiLayerPerceptron , self ) . __init__ ( ** kwargs ) channels = [ input_dims ] + list ( layer_params ) + [ num_classes ] layers = [] for i in range ( len ( channels ) - 1 ): layers . append ( nn . Sequential ( nn . Linear ( channels [ i ], channels [ i + 1 ]), nn . ReLU ())) self . mlp = nn . Sequential ( * layers ) def forward ( self , x ): # x: the feature vector initally read from the data structure, in dimension (N, C, P) x = x . flatten ( start_dim = 1 ) # (N, L), where L = C * P return self . mlp ( x ) Then, we write the get_model and get_loss functions which will be sent into Weaver 's training code. get_model and get_loss function Also see networks/top/mlp_pf.py . We elaborate here on several aspects. Inside get_model , the model is essentially the MLP class we define, and the model_info takes the default definition, including the input/output shape, the dimensions of the dynamic axes for the input/output data shape that will guide the ONNX model exportation. The get_loss function is not changed as in the classification task we always use the cross-entropy loss function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def get_model ( data_config , ** kwargs ): layer_params = ( 2048 , 256 , 256 ) _ , pf_length , pf_features_dims = data_config . input_shapes [ 'pf_features' ] input_dims = pf_length * pf_features_dims num_classes = len ( data_config . label_value ) model = MultiLayerPerceptron ( input_dims , num_classes , layer_params = layer_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () Below show the full structure of the MLP network printed by PyTorch. You will see it in the Weaver output during the training. The full-scale structure of the MLP network MultiLayerPerceptron( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (mlp): Sequential( |0.739 M, 100.000% Params, 0.001 GMac, 100.000% MACs| (0): Sequential( |0.411 M, 55.540% Params, 0.0 GMac, 55.563% MACs| (0): Linear(in_features=400, out_features=1024, bias=True, |0.411 M, 55.540% Params, 0.0 GMac, 55.425% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.138% MACs|) ) (1): Sequential( |0.262 M, 35.492% Params, 0.0 GMac, 35.452% MACs| (0): Linear(in_features=1024, out_features=256, bias=True, |0.262 M, 35.492% Params, 0.0 GMac, 35.418% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (2): Sequential( |0.066 M, 8.899% Params, 0.0 GMac, 8.915% MACs| (0): Linear(in_features=256, out_features=256, bias=True, |0.066 M, 8.899% Params, 0.0 GMac, 8.880% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.035% MACs|) ) (3): Sequential( |0.001 M, 0.070% Params, 0.0 GMac, 0.070% MACs| (0): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.070% Params, 0.0 GMac, 0.069% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in data/top/pf_features.yaml . It defines one input group, pf_features , which takes four variables Etarel , Phirel , E_log , P_log . This is based on our data structure, where these variables are 2D vectors with variable lengths. The length is chosen as 100 in a way that the last dimension (the jet constituent dimension) is always truncated or padded to have length 100. In the following two models (i,e., the DeepAK8 and the ParticleNet model) you will see that the data cards are very similar. The change will only be the way we present the input group(s). DeepAK8 (1D CNN) Note The DeepAK8 tagger is a widely used highly-boosted jet tagging in the CMS community. The design of the model can be found in the CMS paper [ arXiv:2004.08262 ]. The original model is trained on MXNet and its configuration can be found here . We now migrate the model architecture to Weaver and train it on Pytorch. Also, we narrow the multi-class output score to the binary output to adapt our binary classification task (top vs. QCD jet). The model card is given in networks/top/deepak8_pf.py . The DeepAK8 model is inspired by the ResNet architecture. The key ingredient is the ResNet unit constructed by multiple CNN layers with a shortcut connection. First, we define the ResNet unit in the model card. ResNet unit implementation See networks/top/deepak8_pf.py . We elaborate here on several aspects. A ResNet unit is made of two 1D CNNs with batch normalization and ReLU activation function. The shortcut is introduced here by directly adding the input data to the processed data after passing the CNN layers. The shortcut connection help to ease the training for the \"deeper\" model [ arXiv:1512.03385 ]. Note that a trivial linear transformation is applied ( self.conv_sc ) if the feature dimension of the input and output data does not match. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 class ResNetUnit ( nn . Module ): def __init__ ( self , in_channels , out_channels , strides = ( 1 , 1 ), ** kwargs ): super ( ResNetUnit , self ) . __init__ ( ** kwargs ) self . conv1 = nn . Conv1d ( in_channels , out_channels , kernel_size = 3 , stride = strides [ 0 ], padding = 1 ) self . bn1 = nn . BatchNorm1d ( out_channels ) self . conv2 = nn . Conv1d ( out_channels , out_channels , kernel_size = 3 , stride = strides [ 1 ], padding = 1 ) self . bn2 = nn . BatchNorm1d ( out_channels ) self . relu = nn . ReLU () self . dim_match = True if not in_channels == out_channels or not strides == ( 1 , 1 ): # dimensions not match self . dim_match = False self . conv_sc = nn . Conv1d ( in_channels , out_channels , kernel_size = 1 , stride = strides [ 0 ] * strides [ 1 ], bias = False ) def forward ( self , x ): identity = x x = self . conv1 ( x ) x = self . bn1 ( x ) x = self . relu ( x ) x = self . conv2 ( x ) x = self . bn2 ( x ) x = self . relu ( x ) # print('resnet unit', identity.shape, x.shape, self.dim_match) if self . dim_match : return identity + x else : return self . conv_sc ( identity ) + x With the ResNet unit, we construct the DeepAK8 model. The model hyperparameters are chosen as follows. conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] DeepAK8 model implementation See networks/top/deepak8_pf.py . Note that the main architecture is a Pytorch re-implementation of the code here based on the MXNet. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 class ResNet ( nn . Module ): r \"\"\"Parameters ---------- features_dims : int Input feature dimensions. num_classes : int Number of output classes. conv_params : list List of the convolution layer parameters. The first element is a tuple of size 1, defining the transformed feature size for the initial feature convolution layer. The following are tuples of feature size for multiple stages of the ResNet units. Each number defines an individual ResNet unit. \"\"\" def __init__ ( self , features_dims , num_classes , conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )], fc_params = [( 512 , 0.2 )], ** kwargs ): super ( ResNet , self ) . __init__ ( ** kwargs ) self . conv_params = conv_params self . num_stages = len ( conv_params ) - 1 self . fts_conv = nn . Sequential ( nn . Conv1d ( in_channels = features_dims , out_channels = conv_params [ 0 ][ 0 ], kernel_size = 3 , stride = 1 , padding = 1 ), nn . BatchNorm1d ( conv_params [ 0 ][ 0 ]), nn . ReLU ()) # define ResNet units for each stage. Each unit is composed of a sequence of ResNetUnit block self . resnet_units = nn . ModuleDict () for i in range ( self . num_stages ): # stack units[i] layers in this stage unit_layers = [] for j in range ( len ( conv_params [ i + 1 ])): in_channels , out_channels = ( conv_params [ i ][ - 1 ], conv_params [ i + 1 ][ 0 ]) if j == 0 \\ else ( conv_params [ i + 1 ][ j - 1 ], conv_params [ i + 1 ][ j ]) strides = ( 2 , 1 ) if ( j == 0 and i > 0 ) else ( 1 , 1 ) unit_layers . append ( ResNetUnit ( in_channels , out_channels , strides )) self . resnet_units . add_module ( 'resnet_unit_ %d ' % i , nn . Sequential ( * unit_layers )) # define fully connected layers fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param in_chn = conv_params [ - 1 ][ - 1 ] if idx == 0 else fc_params [ idx - 1 ][ 0 ] fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) def forward ( self , x ): # x: the feature vector, (N, C, P) x = self . fts_conv ( x ) for i in range ( self . num_stages ): x = self . resnet_units [ 'resnet_unit_ %d ' % i ]( x ) # (N, C', P'), P'<P due to kernal_size>1 or stride>1 # global average pooling x = x . sum ( dim =- 1 ) / x . shape [ - 1 ] # (N, C') # fully connected x = self . fc ( x ) # (N, out_chn) return x def get_model ( data_config , ** kwargs ): conv_params = [( 32 ,), ( 64 , 64 ), ( 64 , 64 ), ( 128 , 128 )] fc_params = [( 512 , 0.2 )] pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ResNet ( pf_features_dims , num_classes , conv_params = conv_params , fc_params = fc_params ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () Below show the full structure of the DeepAK8 model based on 1D CNN with ResNet. It is printed by PyTorch and you will see it in the Weaver output during training. The full-scale structure of the DeepAK8 architecture ResNet( |0.349 M, 100.000% Params, 0.012 GMac, 100.000% MACs| (fts_conv): Sequential( |0.0 M, 0.137% Params, 0.0 GMac, 0.427% MACs| (0): Conv1d(4, 32, kernel_size=(3,), stride=(1,), padding=(1,), |0.0 M, 0.119% Params, 0.0 GMac, 0.347% MACs|) (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.018% Params, 0.0 GMac, 0.053% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.027% MACs|) ) (resnet_units): ModuleDict( |0.282 M, 80.652% Params, 0.012 GMac, 99.010% MACs| (resnet_unit_0): Sequential( |0.046 M, 13.124% Params, 0.005 GMac, 38.409% MACs| (0): ResNetUnit( |0.021 M, 5.976% Params, 0.002 GMac, 17.497% MACs| (conv1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.006 M, 1.778% Params, 0.001 GMac, 5.175% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) (conv_sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.587% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.003 GMac, 20.912% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 10.296% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.107% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.107% MACs|) ) ) (resnet_unit_1): Sequential( |0.054 M, 15.471% Params, 0.003 GMac, 22.619% MACs| (0): ResNetUnit( |0.029 M, 8.322% Params, 0.001 GMac, 12.163% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 64, kernel_size=(1,), stride=(2,), bias=False, |0.004 M, 1.173% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.025 M, 7.149% Params, 0.001 GMac, 10.456% MACs| (conv1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,), |0.012 M, 3.538% Params, 0.001 GMac, 5.148% MACs|) (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.037% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) (resnet_unit_2): Sequential( |0.182 M, 52.057% Params, 0.005 GMac, 37.982% MACs| (0): ResNetUnit( |0.083 M, 23.682% Params, 0.002 GMac, 17.284% MACs| (conv1): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,), |0.025 M, 7.075% Params, 0.001 GMac, 5.148% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) (conv_sc): Conv1d(64, 128, kernel_size=(1,), stride=(2,), bias=False, |0.008 M, 2.346% Params, 0.0 GMac, 1.707% MACs|) ) (1): ResNetUnit( |0.099 M, 28.375% Params, 0.002 GMac, 20.698% MACs| (conv1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (conv2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,), |0.049 M, 14.114% Params, 0.001 GMac, 10.269% MACs|) (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.073% Params, 0.0 GMac, 0.053% MACs|) (relu): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.053% MACs|) ) ) ) (fc): Sequential( |0.067 M, 19.210% Params, 0.0 GMac, 0.563% MACs| (0): Sequential( |0.066 M, 18.917% Params, 0.0 GMac, 0.555% MACs| (0): Linear(in_features=128, out_features=512, bias=True, |0.066 M, 18.917% Params, 0.0 GMac, 0.551% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.004% MACs|) (2): Dropout(p=0.2, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=512, out_features=2, bias=True, |0.001 M, 0.294% Params, 0.0 GMac, 0.009% MACs|) ) ) The data card is the same as the MLP case, shown in data/top/pf_features.yaml . ParticleNet (DGCNN) Note The ParticleNet model applied to the CMS analysis is provided in networks/particlenet_pf_sv.py , and the data card in data/ak15_points_pf_sv.yaml . Here we use a similar configuration card to deal with the benchmark task. We will elaborate on the ParticleNet model and focus more on the technical side in this section. The model is defined in networks/top/particlenet_pf.py , but it imports some constructor, the EdgeConv block, in weaver/utils/nn/model/ParticleNet.py . The EdgeConv is illustrated in the cartoon. Illustration of the EdgeConv block From an EdgeConv block's point of view, it requires two classes of features as input: the \"coordinates\" and the \"features\". These features are the per point properties, in the 2D shape with dimensions (C, P) , where C is the size of the features (the feature size of \"coordinates\" and the \"features\" can be different, marked as C_pts , C_fts in the following code), and P is the number of points. The block outputs the new features that the model learns, also in the 2D shape with dimensions (C_fts_out, P) . What happens inside the EdgeConv block? And how is the output feature vector transferred from the input features using the topology of the point cloud? The answer is encoded in the edge convolution (EdgeConv). The edge convolution is an analogues convolution method defined on a point cloud, whose shape is given by the \"coordinates\" of points. Specifically, the input \"coordinates\" provide a view of spatial relations of the points in the Euclidean space. It determines the k -nearest neighbouring points for each point that will guide the update of the feature vector of a point. For each point, the updated feature vector is based on the current state of the point and its k neighbours. Guided by this spirit, all features of the point cloud forms a 3D vector with dimensions (C, P, K) , where C is the per-point feature size (e.g., \u03b7 , \u03c6 , p T \uff0c...), P is the number of points, and K the k -NN number. The structured vector is linearly transformed by acting 2D CNN on the feature dimension C . This helps to aggregate the feature information and exploit the correlations of each point with its adjacent points. A shortcut connection is also introduced inspired by the ResNet. Below shows how the EdgeConv structure is implemented in the code. EdgeConv block implementation See weaver/utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several aspects. The EdgeConvBlock takes the feature dimension in_feat , out_feats which are C_fts , C_fts_out we introduced above. The input data vectors to forward() are \"coordinates\" and \"features\" vector, in the dimension of (N, C_pts(C_fts), P) as introduced above. The first dimension is the mini-batch size. self.get_graph_feature() helps to aggregate k -nearest neighbours for each point. The resulting vector is in the dimension of (N, C_fts, P, K) as we discussed above, K being the k -NN number. After convolutions, the per-point features are merged by taking the mean of all k -nearest neighbouring vectors: fts = x . mean ( dim =- 1 ) # (N, C, P) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 class EdgeConvBlock ( nn . Module ): r \"\"\"EdgeConv layer. Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds <https://arxiv.org/pdf/1801.07829>`__\". Can be described as follows: .. math:: x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}( \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)}) where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`. Parameters ---------- in_feat : int Input feature size. out_feat : int Output feature size. batch_norm : bool Whether to include batch normalization on messages. \"\"\" def __init__ ( self , k , in_feat , out_feats , batch_norm = True , activation = True , cpu_mode = False ): super ( EdgeConvBlock , self ) . __init__ () self . k = k self . batch_norm = batch_norm self . activation = activation self . num_layers = len ( out_feats ) self . get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1 self . convs = nn . ModuleList () for i in range ( self . num_layers ): self . convs . append ( nn . Conv2d ( 2 * in_feat if i == 0 else out_feats [ i - 1 ], out_feats [ i ], kernel_size = 1 , bias = False if self . batch_norm else True )) if batch_norm : self . bns = nn . ModuleList () for i in range ( self . num_layers ): self . bns . append ( nn . BatchNorm2d ( out_feats [ i ])) if activation : self . acts = nn . ModuleList () for i in range ( self . num_layers ): self . acts . append ( nn . ReLU ()) if in_feat == out_feats [ - 1 ]: self . sc = None else : self . sc = nn . Conv1d ( in_feat , out_feats [ - 1 ], kernel_size = 1 , bias = False ) self . sc_bn = nn . BatchNorm1d ( out_feats [ - 1 ]) if activation : self . sc_act = nn . ReLU () def forward ( self , points , features ): # points: (N, C_pts, P) # features: (N, C_fts, P) # N: batch size, C: feature size per point, P: number of points topk_indices = knn ( points , self . k ) # (N, P, K) x = self . get_graph_feature ( features , self . k , topk_indices ) # (N, C_fts, P, K) for conv , bn , act in zip ( self . convs , self . bns , self . acts ): x = conv ( x ) # (N, C', P, K) if bn : x = bn ( x ) if act : x = act ( x ) fts = x . mean ( dim =- 1 ) # (N, C, P) # shortcut if self . sc : sc = self . sc ( features ) # (N, C_out, P) sc = self . sc_bn ( sc ) else : sc = features return self . sc_act ( sc + fts ) # (N, C_out, P) With the EdgeConv architecture as the building block, the ParticleNet model is constructed as follow. The ParticleNet model stacks three EdgeConv blocks to construct higher-level features and passing them through the pipeline. The points (i.e., in our case, the particle candidates inside a jet) are not changing, but the per-point \"coordinates\" and \"features\" vectors changes, in both values and dimensions. For the first EdgeConv block, the \"coordinates\" only includes the relative \u03b7 and \u03c6 value of each particle. The \"features\" is a vector with a standard length of 32, which is linearly transformed from the initial feature vectors including the components of relative \u03b7 , \u03c6 , the log of p T , etc. The first EdgeConv block outputs a per-point feature vector of length 64, which is taken as both the \"coordinates\" and \"features\" to the next EdgeConv block. That is to say, the next k -NN is applied on the 64D high-dimensional spatial space to capture the new relations of points learned by the model. This is visualized by the input/output array showing the data flow of the model. We see that this architecture illustrates the stackability of the EdgeConv block, and is the core to the Dynamic Graph CNN (DGCNN), as the model can dynamically change the correlations of each point based on learnable features. A fusion technique is also used by concatenating the three EdgeConv output vectors together (adding the dimensions), instead of using the last EdgeConv output, to form an output vector. This is also one form of shortcut implementations that helps to ease the training for a complex and deep convolutional network model. The concatenated vectors per point are then averaged over points to produce a single 1D vector of the whole point cloud. The vector passes through one fully connected layer, with a dropout rate of p=0.1 to prevent overfitting. Then, in our example, the full network outputs two scores after a softmax, representing the one-hot encoding of the top vs. QCD class. The ParticleNet implementation is shown below. ParticleNet model implementation See utils/nn/model/ParticleNet.py , or the following code block annotated with more comments. We elaborate here on several mean points. The stack of multiple EdgeConv blocks are implemented in for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask The multiple EdgeConv layer parameters are given by conv_params , which takes a list of tuples, each tuple in the format of (K, (C1, C2, C3)) . K for the k -NN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. The fully connected layer parameters are given by fc_params , which takes a list of tuples, each tuple in the format of (n_feat, drop_rate) . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 class ParticleNet ( nn . Module ): r \"\"\"Parameters ---------- input_dims : int Input feature dimensions (C_fts). num_classes : int Number of output classes. conv_params : list List of convolution parameters of EdgeConv blocks, each element in the format of (K, (C1, C2, C3)). K for the kNN number, C1,2,3 for convolution feature sizes of three layers in an EdgeConv block. fc_params: list List of fully connected layer parameters after all EdgeConv blocks, each element in the format of (n_feat, drop_rate) use_fusion: bool If true, concatenates all output features from each EdgeConv before the fully connected layer. use_fts_bn: bool If true, applies a batch norm before feeding to the EdgeConv block. use_counts: bool If true, uses the real count of points instead of the padded size (the max point size). for_inference: bool Whether this is an inference routine. If true, applies a softmax to the output. for_segmentation: bool Whether the model is set up for the point cloud segmentation (instead of classification) task. If true, does not merge the features after the last EdgeConv, and apply Conv1D instead of the linear layer. The output is hence each output_features per point, instead of output_features. \"\"\" def __init__ ( self , input_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , for_inference = False , for_segmentation = False , ** kwargs ): super ( ParticleNet , self ) . __init__ ( ** kwargs ) self . use_fts_bn = use_fts_bn if self . use_fts_bn : self . bn_fts = nn . BatchNorm1d ( input_dims ) self . use_counts = use_counts self . edge_convs = nn . ModuleList () for idx , layer_param in enumerate ( conv_params ): k , channels = layer_param in_feat = input_dims if idx == 0 else conv_params [ idx - 1 ][ 1 ][ - 1 ] self . edge_convs . append ( EdgeConvBlock ( k = k , in_feat = in_feat , out_feats = channels , cpu_mode = for_inference )) self . use_fusion = use_fusion if self . use_fusion : in_chn = sum ( x [ - 1 ] for _ , x in conv_params ) out_chn = np . clip (( in_chn // 128 ) * 128 , 128 , 1024 ) self . fusion_block = nn . Sequential ( nn . Conv1d ( in_chn , out_chn , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( out_chn ), nn . ReLU ()) self . for_segmentation = for_segmentation fcs = [] for idx , layer_param in enumerate ( fc_params ): channels , drop_rate = layer_param if idx == 0 : in_chn = out_chn if self . use_fusion else conv_params [ - 1 ][ 1 ][ - 1 ] else : in_chn = fc_params [ idx - 1 ][ 0 ] if self . for_segmentation : fcs . append ( nn . Sequential ( nn . Conv1d ( in_chn , channels , kernel_size = 1 , bias = False ), nn . BatchNorm1d ( channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) else : fcs . append ( nn . Sequential ( nn . Linear ( in_chn , channels ), nn . ReLU (), nn . Dropout ( drop_rate ))) if self . for_segmentation : fcs . append ( nn . Conv1d ( fc_params [ - 1 ][ 0 ], num_classes , kernel_size = 1 )) else : fcs . append ( nn . Linear ( fc_params [ - 1 ][ 0 ], num_classes )) self . fc = nn . Sequential ( * fcs ) self . for_inference = for_inference def forward ( self , points , features , mask = None ): # print('points:\\n', points) # print('features:\\n', features) if mask is None : mask = ( features . abs () . sum ( dim = 1 , keepdim = True ) != 0 ) # (N, 1, P) points *= mask features *= mask coord_shift = ( mask == 0 ) * 1e9 if self . use_counts : counts = mask . float () . sum ( dim =- 1 ) counts = torch . max ( counts , torch . ones_like ( counts )) # >=1 if self . use_fts_bn : fts = self . bn_fts ( features ) * mask else : fts = features outputs = [] for idx , conv in enumerate ( self . edge_convs ): pts = ( points if idx == 0 else fts ) + coord_shift fts = conv ( pts , fts ) * mask if self . use_fusion : outputs . append ( fts ) if self . use_fusion : fts = self . fusion_block ( torch . cat ( outputs , dim = 1 )) * mask # assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0) if self . for_segmentation : x = fts else : if self . use_counts : x = fts . sum ( dim =- 1 ) / counts # divide by the real counts else : x = fts . mean ( dim =- 1 ) output = self . fc ( x ) if self . for_inference : output = torch . softmax ( output , dim = 1 ) # print('output:\\n', output) return output Above are the capsulation of all ParticleNet building blocks. Eventually, we have the model defined in the model card networks/benchmark/particlenet.py , in the ParticleNetTagger1Path class, meaning we only use the ParticleNet pipeline that deals with one set of the point cloud (i.e., the particle candidates). Info Two sets of point clouds in the CMS application, namely the particle-flow candidates and secondary vertices, are used. This requires special handling to merge the clouds before feeding them to the first layer of EdgeConv. ParticleNet model config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 import torch import torch.nn as nn from utils.nn.model.ParticleNet import ParticleNet , FeatureConv class ParticleNetTagger1Path ( nn . Module ): def __init__ ( self , pf_features_dims , num_classes , conv_params = [( 7 , ( 32 , 32 , 32 )), ( 7 , ( 64 , 64 , 64 ))], fc_params = [( 128 , 0.1 )], use_fusion = True , use_fts_bn = True , use_counts = True , pf_input_dropout = None , for_inference = False , ** kwargs ): super ( ParticleNetTagger1Path , self ) . __init__ ( ** kwargs ) self . pf_input_dropout = nn . Dropout ( pf_input_dropout ) if pf_input_dropout else None self . pf_conv = FeatureConv ( pf_features_dims , 32 ) self . pn = ParticleNet ( input_dims = 32 , num_classes = num_classes , conv_params = conv_params , fc_params = fc_params , use_fusion = use_fusion , use_fts_bn = use_fts_bn , use_counts = use_counts , for_inference = for_inference ) def forward ( self , pf_points , pf_features , pf_mask ): if self . pf_input_dropout : pf_mask = ( self . pf_input_dropout ( pf_mask ) != 0 ) . float () pf_points *= pf_mask pf_features *= pf_mask return self . pn ( pf_points , self . pf_conv ( pf_features * pf_mask ) * pf_mask , pf_mask ) def get_model ( data_config , ** kwargs ): conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] use_fusion = True pf_features_dims = len ( data_config . input_dicts [ 'pf_features' ]) num_classes = len ( data_config . label_value ) model = ParticleNetTagger1Path ( pf_features_dims , num_classes , conv_params , fc_params , use_fusion = use_fusion , use_fts_bn = kwargs . get ( 'use_fts_bn' , False ), use_counts = kwargs . get ( 'use_counts' , True ), pf_input_dropout = kwargs . get ( 'pf_input_dropout' , None ), for_inference = kwargs . get ( 'for_inference' , False ) ) model_info = { 'input_names' : list ( data_config . input_names ), 'input_shapes' :{ k :(( 1 ,) + s [ 1 :]) for k , s in data_config . input_shapes . items ()}, 'output_names' :[ 'softmax' ], 'dynamic_axes' :{ ** { k :{ 0 : 'N' , 2 : 'n_' + k . split ( '_' )[ 0 ]} for k in data_config . input_names }, ** { 'softmax' :{ 0 : 'N' }}}, } print ( model , model_info ) print ( data_config . input_shapes ) return model , model_info def get_loss ( data_config , ** kwargs ): return torch . nn . CrossEntropyLoss () The most important parameters are conv_params and fc_params , which decides the model parameters of EdgeConv blocks and the fully connected layer. See details in the above \"ParticleNet model implementation\" box. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] A full structure printed from PyTorch is shown below. It will appear in the Weaver output during training. ParticleNet full-scale structure ParticleNetTagger1Path( |0.577 M, 100.000% Params, 0.441 GMac, 100.000% MACs| (pf_conv): FeatureConv( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (conv): Sequential( |0.0 M, 0.035% Params, 0.0 GMac, 0.005% MACs| (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.001% Params, 0.0 GMac, 0.000% MACs|) (1): Conv1d(4, 32, kernel_size=(1,), stride=(1,), bias=False, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.011% Params, 0.0 GMac, 0.001% MACs|) (3): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) ) (pn): ParticleNet( |0.577 M, 99.965% Params, 0.441 GMac, 99.995% MACs| (edge_convs): ModuleList( |0.305 M, 52.823% Params, 0.424 GMac, 96.047% MACs| (0): EdgeConvBlock( |0.015 M, 2.575% Params, 0.021 GMac, 4.716% MACs| (convs): ModuleList( |0.012 M, 2.131% Params, 0.02 GMac, 4.456% MACs| (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) (2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.004 M, 0.710% Params, 0.007 GMac, 1.485% MACs|) ) (bns): ModuleList( |0.0 M, 0.067% Params, 0.001 GMac, 0.139% MACs| (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.046% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.0 GMac, 0.070% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.023% MACs|) ) (sc): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False, |0.002 M, 0.355% Params, 0.0 GMac, 0.046% MACs|) (sc_bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.022% Params, 0.0 GMac, 0.003% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.001% MACs|) ) (1): EdgeConvBlock( |0.058 M, 10.121% Params, 0.081 GMac, 18.437% MACs| (convs): ModuleList( |0.049 M, 8.523% Params, 0.079 GMac, 17.825% MACs| (0): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) (2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.016 M, 2.841% Params, 0.026 GMac, 5.942% MACs|) ) (bns): ModuleList( |0.001 M, 0.133% Params, 0.001 GMac, 0.279% MACs| (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.093% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.139% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.046% MACs|) ) (sc): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False, |0.008 M, 1.420% Params, 0.001 GMac, 0.186% MACs|) (sc_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.0 M, 0.044% Params, 0.0 GMac, 0.006% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.003% MACs|) ) (2): EdgeConvBlock( |0.231 M, 40.128% Params, 0.322 GMac, 72.894% MACs| (convs): ModuleList( |0.197 M, 34.091% Params, 0.315 GMac, 71.299% MACs| (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) (2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False, |0.066 M, 11.364% Params, 0.105 GMac, 23.766% MACs|) ) (bns): ModuleList( |0.002 M, 0.266% Params, 0.002 GMac, 0.557% MACs| (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.001 GMac, 0.186% MACs|) ) (acts): ModuleList( |0.0 M, 0.000% Params, 0.001 GMac, 0.279% MACs| (0): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.093% MACs|) ) (sc): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False, |0.033 M, 5.682% Params, 0.003 GMac, 0.743% MACs|) (sc_bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.012% MACs|) (sc_act): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.006% MACs|) ) ) (fusion_block): Sequential( |0.173 M, 29.963% Params, 0.017 GMac, 3.925% MACs| (0): Conv1d(448, 384, kernel_size=(1,), stride=(1,), bias=False, |0.172 M, 29.830% Params, 0.017 GMac, 3.899% MACs|) (1): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, |0.001 M, 0.133% Params, 0.0 GMac, 0.017% MACs|) (2): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.009% MACs|) ) (fc): Sequential( |0.099 M, 17.179% Params, 0.0 GMac, 0.023% MACs| (0): Sequential( |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs| (0): Linear(in_features=384, out_features=256, bias=True, |0.099 M, 17.090% Params, 0.0 GMac, 0.022% MACs|) (1): ReLU(|0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) (2): Dropout(p=0.1, inplace=False, |0.0 M, 0.000% Params, 0.0 GMac, 0.000% MACs|) ) (1): Linear(in_features=256, out_features=2, bias=True, |0.001 M, 0.089% Params, 0.0 GMac, 0.000% MACs|) ) ) ) The data card is shown in data/top/pf_features.yaml , given in a similar way as in the MLP example. Here we group the inputs into three classes: pf_points , pf_features and pf_masks . They correspond to the forward(self, pf_points, pf_features, pf_mask) prototype of our nn.Module model, and will send in these 2D vectors in the mini-batch size for each iteration during training/prediction. ParticleNet data config 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 selection : ### use `&`, `|`, `~` for logical operations on numpy arrays ### can use functions from `math`, `np` (numpy), and `awkward` in the expression new_variables : ### [format] name: formula ### can use functions from `math`, `np` (numpy), and `awkward` in the expression pf_mask : awkward.JaggedArray.ones_like(E) is_bkg : np.logical_not(is_signal_new) preprocess : ### method: [manual, auto] - whether to use manually specified parameters for variable standardization method : manual ### data_fraction: fraction of events to use when calculating the mean/scale for the standardization data_fraction : inputs : pf_points : length : 100 vars : - [ PX , 0 , 0.05 ] - [ PY , 0 , 0.05 ] - [ PZ , 0 , 0.05 ] pf_features : length : 100 vars : ### [format 1]: var_name (no transformation) ### [format 2]: [var_name, ### subtract_by(optional, default=None, no transf. if preprocess.method=manual, auto transf. if preprocess.method=auto), ### multiply_by(optional, default=1), ### clip_min(optional, default=-5), ### clip_max(optional, default=5), ### pad_value(optional, default=0)] - [ PX , 0 , 0.05 ] - [ PY , 0 , 0.05 ] - [ PZ , 0 , 0.05 ] - [ E_log , 2 , 1 ] pf_mask : length : 100 vars : - pf_mask labels : ### type can be `simple`, `custom` ### [option 1] use `simple` for binary/multi-class classification, then `value` is a list of 0-1 labels type : simple value : [ is_signal_new , is_bkg ] ### [option 2] otherwise use `custom` to define the label, then `value` is a map # type: custom # value: # target_mass: np.where(fj_isQCD, fj_genjet_sdmass, fj_gen_mass) observers : - origIdx - idx - E_tot - PX_tot - PY_tot - PZ_tot - P_tot - Eta_tot - Phi_tot # weights: ### [option 1] use precomputed weights stored in the input files # use_precomputed_weights: true # weight_branches: [weight, class_weight] ### [option 2] compute weights on-the-fly using reweighting histograms Above, we analyze the three networks in a detailed manner. As a summary, we draw the three network architectures in the following three cartoons for a more overall and comprehensive understanding. The full architecture of the proof-of-concept multi-layer perception model. The full architecture of the DeepAK8 model, which is based on 1D CNN with ResNet architecture. The full architecture of the ParticleNet model, which is based on DGCNN and EdgeConv. The model and data configuration cards, the number of parameters, and computational complexity are summarized in the following table. Note that we'll refer to the shell variables provided here in the following training example. Model ${prefix} ${model_config} ${data_config} Parameters Computational complexity MLP mlp ../networks/top/mlp_pf.py ../data/top/pf_features.yml 739k 0.001 GMac DeepAK8 (1D CNN) deepak8 ../networks/top/deepak8.py ../data/top/pf_features.yml 349k 0.012 GMac ParticleNet (DGCNN) particlenet ../networks/top/particlenet_pf.py ../data/top/pf_points_features.yml 577k 0.441 GMac","title":"1. Build models in Weaver"},{"location":"inference/particlenet.html#2-start-training","text":"Now we train the three neural networks based on the provided model and data configurations. Here we present three ways of training. For readers who have a local machine with CUDA GPUs, please try out training on the local GPUs. Readers who would like to try on CPUs can also refer to the local GPU instruction. It is also possible to borrow the GPU resources from the lxplus condor or CMS-connect. Please find in the following that meets your situation. Train on local GPUs The three networks can be trained with a universal script. Enter the weaver base folder and run the following command. Note that ${data_config} , ${model_config} , and ${prefix} refers to the value in the above table for each example, and the fake path should be replaced with the correct one. python train . py \\ -- data - train '<path-to-samples>/prep/top_train_*.awkd' \\ -- data - val '<path-to-samples>/prep/top_val_*.awkd' \\ -- fetch - by - file -- fetch - step 1 \\ -- num - workers 3 \\ -- data - config data / benchmark / $ { data_config } \\ -- network - config networks / benchmark / $ { model_config } \\ -- model - prefix output / $ { prefix } \\ -- gpus 0 , 1 -- batch - size 1024 -- start - lr 5e-3 -- num - epochs 20 -- optimizer ranger \\ -- log output / $ { prefix } . train . log Here are several caveats running this command. Warning --fetch-by-file is very important when you have only a few input files. Please see Weaver README for more details on this argument. A common situation is that when you have only one input file that includes both signal and backgrounds, but events are not well mixed. Then, it is a must to use --fetch-by-file , because otherwise, only a small collection of events on the top are read into the memory and sending to the mini-batch after shuffling. In such cases, events can have a strong bias in different categories (the most extreme case is that it only contains one category) and may confuse the training. By using --fetch-by-file , all events in a file are read into the memory, shuffled properly and sent to the mini-batch. Specify GPUS based on their device IDs. For running on CPUs, please use --gpu '' . Then, predict the score on the test datasets using the best model: python train . py -- predict \\ -- data - test '<path-to-samples>/prep/top_test_*.awkd' \\ -- num - workers 3 \\ -- data - config data / benchmark / $ { data_config } \\ -- network - config networks / benchmark / $ { model_config } \\ -- model - prefix output / $ { prefix } _best_epoch_state . pt \\ -- gpus 0 , 1 -- batch - size 1024 \\ -- predict - output output / $ { prefix } _predict . root Use GPUs on lxplus HTCondor On lxplus HTCondor, the GPU(s) can be booked via the arguments request_gpus . To get familiar with the GPU service, please refer to the documentation here . While it is not possible to test the script locally, you can try out the condor_ssh_to_job command to connect to the remote condor machine that runs the jobs. This interesting feature will help you with debugging or monitoring the condor job. Here we provide the example executed script and the condor submitted file for the training and predicting task. Create the following two files: The executable: run.sh Still, please remember to specify ${data_config} , ${model_config} , and ${prefix} as shown in the above table, and replace the fake path with the correct one. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 #!/bin/bash WORKDIR = ` pwd ` # Download miniconda wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda_install.sh bash miniconda_install.sh -b -p ${ WORKDIR } /miniconda export PATH = $WORKDIR /miniconda/bin: $PATH pip install numpy pandas scikit-learn scipy matplotlib tqdm PyYAML pip install uproot3 awkward0 lz4 xxhash pip install tables pip install onnxruntime-gpu pip install torch # CUDA environment setup export PATH = $PATH :/usr/local/cuda-10.2/bin export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/usr/local/cuda-10.2/lib64 export LIBRARY_PATH = $LIBRARY_PATH :/usr/local/cuda-10.2/lib64 # Clone weaver git clone https://github.com/hqucms/weaver.git cd weaver/ mkdir output # Training python train.py \\ --data-train '<path-to-samples>/prep/top_train_*.awkd' \\ --data-val '<path-to-samples>/prep/top_val_*.awkd' \\ --fetch-by-file --fetch-step 1 \\ --num-workers 3 \\ --data-config data/benchmark/ ${ data_config } \\ --network-config networks/benchmark/ ${ model_config } \\ --model-prefix output/ ${ prefix } \\ --gpus 0 --batch-size 1024 --start-lr 5e-3 --num-epochs 20 --optimizer ranger \\ --log output/ ${ prefix } .train.log # Predicting score python train.py --predict \\ --data-test '<path-to-samples>/prep/top_test_*.awkd' \\ --num-workers 3 \\ --data-config data/benchmark/ ${ data_config } \\ --network-config networks/benchmark/ ${ model_config } \\ --model-prefix output/ ${ prefix } _best_epoch_state.pt \\ --gpus 0 --batch-size 1024 \\ --predict-output output/ ${ prefix } _predict.root HTCondor submitted file: submit.sub 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Universe = vanilla executable = run.sh arguments = output = logs/$(ClusterId).$(ProcId).out error = logs/$(ClusterId).$(ProcId).err log = logs/$(ClusterId).log should_transfer_files = YES when_to_transfer_output = ON_EXIT_OR_EVICT transfer_output_files = weaver/output transfer_output_remaps = \"output = output.$(ClusterId).$(ProcId)\" request_GPUs = 1 request_CPUs = 2 +MaxRuntime = 604800 queue Make the run.sh script an executable, then submit the job. chmod +x run.sh condor_submit submit.sh The weaver/output directory will be transferred back. Use GPUs on CMS Connect CMS Connect provides several GPU nodes. ...","title":"2. Start training!"},{"location":"inference/particlenet.html#3-evaluation-of-models","text":"In the output folder, we find the trained PyTorch models after every epoch and the log file that records the loss and accuracy in the runtime. The predict step also produces a predicted root file in the output folder, including the truth label, the predicted store, and several observer variables we provided in the data card. With the predicted root file, we make the ROC curve comparing the performance of the three trained models. Here is the result from my training: Model AUC Accuracy 1/ e B (@ e S =0.3) MLP 0.961 0.898 186 DeepAK8 (1D CNN) 0.979 0.927 585 ParticleNet (DGCNN) 0.984 0.936 1030 We see that the ParticleNet model shows an outstanding performance in this classification task. Besides, the DeepAK8 and ParticleNet results are similar to the benchmark values found in the gDoc . We address that the performance can be further improved by fine-tuning on the model, and by taking the average predicted score from an ensemble of the trained model with different initial parametrization - a well known ML technique to pursue an extra few percent of improvements.","title":"3. Evaluation of models"},{"location":"inference/particlenet.html#tuning-the-particlenet-model","text":"When it comes to the real application of any DNN model, tunning the hyperparameters is an important path towards a better performance. In this section, we provide some tips on the ParticleNet model tunning. For a more detailed discussion on this topic, see more in the \"validation\" chapter in the documentation.","title":"Tuning the ParticleNet model"},{"location":"inference/particlenet.html#1-choices-on-the-optimizer-and-the-learning-rate","text":"The optimizer decides how our neural network update all its parameters, and the learning rate means how fast the parameters changes in one training iteration. Learning rate is the most important hyperparameter to choose from before concrete training is done. Here we quote from a suggested strategy: if you only have the opportunity to optimize one hyperparameter, choose the learning rate. The optimizer is also important because a wiser strategy usually means avoid the zig-zagging updating route, avoid falling into the local minima and even adapting different strategies for the fast-changing parameters and the slow ones. Adam (and its several variations) is a widely used optimizer. Another recently developed advanced optimizer is Ranger that combines RAdam and LookAhead. However, one should note that the few percent level improvement by using different optimizers is likely to be smeared by an unoptimized learning rate. The above training scheme uses a start learning rate of 5e-3, and Ranger as the optimizer. It uses a flat+decay schedular, in a way that the LR starts to decay after processing 70% of epochs, and gradually reduce to 0.1 of its original value when nearing the completion of all epochs. First, we note that the current case is already well optimized. Therefore, by simply reuse the current choice, the training will converge to a stable result in general. But it is always good in practice to test several choices of the optimizer and reoptimize the learning rate. Weaver integrates multiple optimizers. In the above training command, we use --optimizer ranger to adopt the Ranger optimizer. It is also possible to switch to --optimizer adam or --optimizer adamW . Weaver also provides the interface to optimize the learning rate before real training is performed. In the ParticleNet model training, we append --lr-finder 5e-4,5e-1,1000 in the command, then a specific learning-rate finder program will be launched. This setup scans over the LR from 5e-4 to 5e-1 by applying 1000 mini-batches of training. It outputs a plot showing the training loss for different starting learning rates. In general, a lower training loss means a better choice of the learning rate parameter. Below shows the results from LR finder by specifying --lr-finder 5e-4,5e-1,200 (upper) and --lr-finder 5e-4,5e-1,1000 (lower), for the --optimizer adamW (left) and the --optimizer ranger (right) case. The plots show that the training loss forms a basin over a wide range of the learning rate. Therefore, the LR finder only provides a rough estimation. But it is a good attempt to first run the LR finder to have an overall feeling. Besides, we should be aware that different optimizer takes different optimal LR values. As can be seen here, the AdamW in general requires a small LR than Ranger. In the choice of Ranger, we further optimize the LR based on the full training of our model. The reason is that although training the first set of 1000 mini-batches cannot manifest their true impact, in the real training when we have a scheduler to adapt smaller LR gradually, the difference of starting LR choices may take effect. To monitor the full training/evaluation accuracy and the loss for each mini-batch, we can draw support from a nicely integrated utility, TensorBoard, to employ real-time monitoring. To activate TensorBoard, append (note that replace ${prefix} according to the above table) --tensorboard ${ prefix } to the training command, then direct to https://localhost:6008 for the TensorBoard UI. The below plots show the training and evaluation loss, in our standard choice with LR being 5e-3, and in the case of a small LR 2e-3 and a large LR 1e-2. Note that all tested LR values are within the basin in the LR finder plots. We see that in the evaluated loss plot, the standard LR outperforms two variational choices. The reason may be that a larger LR finds difficulty in converging to the global minima, while a smaller LR may not be adequate to reach the minima point in a journey of 20 epochs. Overall, we see 5e-3 as a good choice as the starting LR for the Ranger optimizer.","title":"1. Choices on the optimizer and the learning rate"},{"location":"inference/particlenet.html#2-optimize-the-model","text":"In practice, tuning the model size is also an important task. By concept, a smaller model tends to have unsatisfactory performance due to the limited ability to learn many local features. As the model size goes up, the performance will climb to some extent, but may further decrease due to the network \"degradation\" (deeper models have difficulty learning features). Besides, a heavier model may also cause the overfitting issue. In practice, it also leads to larger inference time which is the main concern when coming to real applications. For the ParticleNet model case, we also test between a smaller and larger variation of the model size. Recall that the original model is defined by the following layer parameters. conv_params = [ ( 16 , ( 64 , 64 , 64 )), ( 16 , ( 128 , 128 , 128 )), ( 16 , ( 256 , 256 , 256 )), ] fc_params = [( 256 , 0.1 )] We can replace the code block with ec_k = kwargs . get ( 'ec_k' , 16 ) ec_c1 = kwargs . get ( 'ec_c1' , 64 ) ec_c2 = kwargs . get ( 'ec_c2' , 128 ) ec_c3 = kwargs . get ( 'ec_c3' , 256 ) fc_c , fc_p = kwargs . get ( 'fc_c' , 256 ), kwargs . get ( 'fc_p' , 0.1 ) conv_params = [ ( ec_k , ( ec_c1 , ec_c1 , ec_c1 )), ( ec_k , ( ec_c2 , ec_c2 , ec_c2 )), ( ec_k , ( ec_c3 , ec_c3 , ec_c3 )), ] fc_params = [( fc_c , fc_p )] Then we have the ability to tune the model parameters from the command line. Append the extra arguments in the training command --network-option ec_k 32 --network-option ec_c1 128 --network-option ec_c2 192 --network-option ec_c3 256 and the model parameters will take the new values as specified. We test over two cases, one with the above setting to enlarge the model, and another by using --network-option ec_c1 64 --network-option ec_c2 64 --network-option ec_c3 96 to adopt a lite version. The Tensorboard monitoring plots in the training/evaluation loss is shown as follows. We see that the \"heavy\" model reaches even smaller training loss, meaning that the model does not meet the degradation issue yet. However, the evaluation loss is not catching up with the training loss, showing some degree of overtraining in this scheme. From the evaluation result, we see no improvement by moving to a heavy model. Above, we discuss in a very detailed manner on various attempts we can make to optimize the model. We hope the practical experiences presented here will help readers develop and deploy the complex ML model.","title":"2. Optimize the model"},{"location":"inference/performance.html","text":"Performance of inference tools \u00b6","title":"Performance"},{"location":"inference/performance.html#performance-of-inference-tools","text":"","title":"Performance of inference tools"},{"location":"inference/sonic_triton.html","text":"Service-based inference with Triton/Sonic \u00b6 This page is still under construction. For the moment, please see the Sonic+Triton tutorial given as part of the Machine Learning HATS@LPC 2021. Link to Indico agenda Slides Exercise twiki","title":"Sonic/Triton"},{"location":"inference/sonic_triton.html#service-based-inference-with-tritonsonic","text":"This page is still under construction. For the moment, please see the Sonic+Triton tutorial given as part of the Machine Learning HATS@LPC 2021. Link to Indico agenda Slides Exercise twiki","title":"Service-based inference with Triton/Sonic"},{"location":"inference/standalone.html","text":"Todo. Idea: Working w/ TF+ROOT standalone (outside of CMSSW)","title":"Standalone framework"},{"location":"inference/swan_aws.html","text":"Todo. Ideas: best practices cost model instance priving need to log out monitoring madatory","title":"SWAN + AWS"},{"location":"inference/tensorflow1.html","text":"Direct inference with TensorFlow 1 \u00b6 While it is technically still possible to use TensorFlow 1, this version of TensorFlow is quite old and is no longer supported by CMSSW. We highly recommend that you update your model to TensorFlow 2 and follow the integration guide in the Inference/Direct inference/TensorFlow 2 documentation.","title":"TensorFlow 1"},{"location":"inference/tensorflow1.html#direct-inference-with-tensorflow-1","text":"While it is technically still possible to use TensorFlow 1, this version of TensorFlow is quite old and is no longer supported by CMSSW. We highly recommend that you update your model to TensorFlow 2 and follow the integration guide in the Inference/Direct inference/TensorFlow 2 documentation.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow2.html","text":"Direct inference with TensorFlow 2 \u00b6 TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions. Software setup \u00b6 To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml Saving your model \u00b6 After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph . Inference in CMSSW \u00b6 The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended . CMSSW module setup \u00b6 If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export> Single-threaded inference \u00b6 Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ... 2. Initialize objects \u00b6 1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef ); 3. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 4. Cleanup \u00b6 1 2 tensorflow :: closeSession ( session ); delete graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Multi-threaded inference \u00b6 Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h . 2. Define and use the cache \u00b6 The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details. 3. Initialize objects \u00b6 1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef ); 4. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 5. Cleanup \u00b6 1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Optimization \u00b6 Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist . Reusing tensors \u00b6 In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call. Tensor data access via pointers \u00b6 As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } } Inter- and intra-operation parallelism \u00b6 Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" ); Miscellaneous \u00b6 Logging \u00b6 By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet. Links and further reading \u00b6 cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"TensorFlow 2"},{"location":"inference/tensorflow2.html#direct-inference-with-tensorflow-2","text":"TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.3. See the guide on inference with TensorFlow 1 for earlier versions.","title":"Direct inference with TensorFlow 2"},{"location":"inference/tensorflow2.html#software-setup","text":"To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml","title":"Software setup"},{"location":"inference/tensorflow2.html#saving-your-model","text":"After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph .","title":"Saving your model"},{"location":"inference/tensorflow2.html#inference-in-cmssw","text":"The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended .","title":"Inference in CMSSW"},{"location":"inference/tensorflow2.html#cmssw-module-setup","text":"If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export>","title":"CMSSW module setup"},{"location":"inference/tensorflow2.html#single-threaded-inference","text":"Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup.","title":"Single-threaded inference"},{"location":"inference/tensorflow2.html#1-includes","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ...","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-initialize-objects","text":"1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef );","title":"2. Initialize objects"},{"location":"inference/tensorflow2.html#3-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"3. Inference"},{"location":"inference/tensorflow2.html#4-cleanup","text":"1 2 tensorflow :: closeSession ( session ); delete graphDef ;","title":"4. Cleanup"},{"location":"inference/tensorflow2.html#full-example","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#multi-threaded-inference","text":"Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup.","title":"Multi-threaded inference"},{"location":"inference/tensorflow2.html#1-includes_1","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h .","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-define-and-use-the-cache","text":"The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details.","title":"2. Define and use the cache"},{"location":"inference/tensorflow2.html#3-initialize-objects","text":"1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef );","title":"3. Initialize objects"},{"location":"inference/tensorflow2.html#4-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"4. Inference"},{"location":"inference/tensorflow2.html#5-cleanup","text":"1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ;","title":"5. Cleanup"},{"location":"inference/tensorflow2.html#full-example_1","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (){}; static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addWithDefaultLabel ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#optimization","text":"Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist .","title":"Optimization"},{"location":"inference/tensorflow2.html#reusing-tensors","text":"In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call.","title":"Reusing tensors"},{"location":"inference/tensorflow2.html#tensor-data-access-via-pointers","text":"As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } }","title":"Tensor data access via pointers"},{"location":"inference/tensorflow2.html#inter-and-intra-operation-parallelism","text":"Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" );","title":"Inter- and intra-operation parallelism"},{"location":"inference/tensorflow2.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"inference/tensorflow2.html#logging","text":"By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet.","title":"Logging"},{"location":"inference/tensorflow2.html#links-and-further-reading","text":"cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API Authors: Marcel Rieger","title":"Links and further reading"},{"location":"inference/tfaas.html","text":"TensorFlow as a Service \u00b6 TensorFlow as a Service (TFaas) was developed as a general purpose service which can be deployed on any infrastruction from personal laptop, VM, to cloud infrastructure, inculding kubernetes/docker based ones. The main repository contains all details about the service, including install , end-to-end example , and demo . For CERN users we already deploy TFaaS on the following URL: https://cms-tfaas.cern.ch It can be used by CMS members using any HTTP based client. For example, here is a basic access from curl client: curl -k https://cms-tfaas.cern.ch/models [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890554036 +0000 UTC m=+600537.976386186\" }, { \"name\": \"test_luca_1024\", \"model\": \"saved_model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_input_1:0\", \"outputNode\": \"dense_3/Sigmoid:0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890776518 +0000 UTC m=+600537.976608672\" }, { \"name\": \"vk\", \"model\": \"model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890903234 +0000 UTC m=+600537.976735378\" } ] The following APIs are available: - /upload to push your favorite TF model to TFaaS server either for Form or as tar-ball bundle, see examples below - /delete to delete your TF model from TFaaS server - /models to view existing TF models on TFaaS server - /predict/json to serve TF model predictions in JSON data-format - /predict/proto to serve TF model predictions in ProtoBuffer data-format - /predict/image to serve TF model predictions forimages in JPG/PNG formats \u2780 look-up your favorite model \u00b6 You may easily look-up your ML model from TFaaS server, e.g. curl https://cms-tfaas.cern.ch/models # possible output may looks like this [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-11-08 20:07:18.397487027 +0000 UTC m=+2091094.457327022\" } ... ] The provided /models API will list the name of the model, its file name, labels file, possible options, input and output nodes, description and proper timestamp when it was added to TFaaS repository \u2781 upload your TF model to TFaaS server \u00b6 If your model is not in TFaaS server you may easily add it as following: # example of image based model upload curl -X POST https://cms-tfaas.cern.ch/upload -F 'name=ImageModel' -F 'params=@/path/params.json' -F 'model=@/path/tf_model.pb' -F 'labels=@/path/labels.txt' # example of TF pb file upload curl -s -X POST https://cms-tfaas.cern.ch/upload \\ -F 'name=vk' -F 'params=@/path/params.json' \\ -F 'model=@/path/model.pb' -F 'labels=@/path/labels.txt' # example of bundle upload produce with Keras TF # here is our saved model area ls model assets saved_model.pb variables # we can create tarball and upload it to TFaaS via bundle end-point tar cfz model.tar.gz model curl -X POST -H \"Content-Encoding: gzip\" \\ -H \"content-type: application/octet-stream\" \\ --data-binary @/path/models.tar.gz https://cms-tfaas.cern.ch/upload \u2782 get your predictions \u00b6 Finally, you may obtain predictions from your favorite model by using proper API, e.g. # obtain predictions from your ImageModel curl https://cms-tfaas.cern.ch/image -F 'image=@/path/file.png' -F 'model=ImageModel' # obtain predictions from your TF based model cat input.json {\"keys\": [...], \"values\": [...], \"model\":\"model\"} # call to get predictions from /json end-point using input.json curl -s -X POST -H \"Content-type: application/json\" \\ -d@/path/input.json https://cms-tfaas.cern.ch/json Fore more information please visit curl client page. TFaaS interface \u00b6 Clients communicate with TFaaS via HTTP protocol. See examples for Curl , Python and C++ clients. TFaaS benchmarks \u00b6 Benchmark results on CentOS, 24 cores, 32GB of RAM serving DL NN with 42x128x128x128x64x64x1x1 architecture (JSON and ProtoBuffer formats show similar performance): - 400 req/sec for 100 concurrent clients, 1000 requests in total - 480 req/sec for 200 concurrent clients, 5000 requests in total For more information please visit bencmarks page.","title":"TFaaS"},{"location":"inference/tfaas.html#tensorflow-as-a-service","text":"TensorFlow as a Service (TFaas) was developed as a general purpose service which can be deployed on any infrastruction from personal laptop, VM, to cloud infrastructure, inculding kubernetes/docker based ones. The main repository contains all details about the service, including install , end-to-end example , and demo . For CERN users we already deploy TFaaS on the following URL: https://cms-tfaas.cern.ch It can be used by CMS members using any HTTP based client. For example, here is a basic access from curl client: curl -k https://cms-tfaas.cern.ch/models [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890554036 +0000 UTC m=+600537.976386186\" }, { \"name\": \"test_luca_1024\", \"model\": \"saved_model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_input_1:0\", \"outputNode\": \"dense_3/Sigmoid:0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890776518 +0000 UTC m=+600537.976608672\" }, { \"name\": \"vk\", \"model\": \"model.pb\", \"labels\": \"labels.txt\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-10-22 14:04:52.890903234 +0000 UTC m=+600537.976735378\" } ] The following APIs are available: - /upload to push your favorite TF model to TFaaS server either for Form or as tar-ball bundle, see examples below - /delete to delete your TF model from TFaaS server - /models to view existing TF models on TFaaS server - /predict/json to serve TF model predictions in JSON data-format - /predict/proto to serve TF model predictions in ProtoBuffer data-format - /predict/image to serve TF model predictions forimages in JPG/PNG formats","title":"TensorFlow as a Service"},{"location":"inference/tfaas.html#look-up-your-favorite-model","text":"You may easily look-up your ML model from TFaaS server, e.g. curl https://cms-tfaas.cern.ch/models # possible output may looks like this [ { \"name\": \"luca\", \"model\": \"prova.pb\", \"labels\": \"labels.csv\", \"options\": null, \"inputNode\": \"dense_1_input\", \"outputNode\": \"output_node0\", \"description\": \"\", \"timestamp\": \"2021-11-08 20:07:18.397487027 +0000 UTC m=+2091094.457327022\" } ... ] The provided /models API will list the name of the model, its file name, labels file, possible options, input and output nodes, description and proper timestamp when it was added to TFaaS repository","title":"&#10112; look-up your favorite model"},{"location":"inference/tfaas.html#upload-your-tf-model-to-tfaas-server","text":"If your model is not in TFaaS server you may easily add it as following: # example of image based model upload curl -X POST https://cms-tfaas.cern.ch/upload -F 'name=ImageModel' -F 'params=@/path/params.json' -F 'model=@/path/tf_model.pb' -F 'labels=@/path/labels.txt' # example of TF pb file upload curl -s -X POST https://cms-tfaas.cern.ch/upload \\ -F 'name=vk' -F 'params=@/path/params.json' \\ -F 'model=@/path/model.pb' -F 'labels=@/path/labels.txt' # example of bundle upload produce with Keras TF # here is our saved model area ls model assets saved_model.pb variables # we can create tarball and upload it to TFaaS via bundle end-point tar cfz model.tar.gz model curl -X POST -H \"Content-Encoding: gzip\" \\ -H \"content-type: application/octet-stream\" \\ --data-binary @/path/models.tar.gz https://cms-tfaas.cern.ch/upload","title":"&#10113; upload your TF model to TFaaS server"},{"location":"inference/tfaas.html#get-your-predictions","text":"Finally, you may obtain predictions from your favorite model by using proper API, e.g. # obtain predictions from your ImageModel curl https://cms-tfaas.cern.ch/image -F 'image=@/path/file.png' -F 'model=ImageModel' # obtain predictions from your TF based model cat input.json {\"keys\": [...], \"values\": [...], \"model\":\"model\"} # call to get predictions from /json end-point using input.json curl -s -X POST -H \"Content-type: application/json\" \\ -d@/path/input.json https://cms-tfaas.cern.ch/json Fore more information please visit curl client page.","title":"&#10114; get your predictions"},{"location":"inference/tfaas.html#tfaas-interface","text":"Clients communicate with TFaaS via HTTP protocol. See examples for Curl , Python and C++ clients.","title":"TFaaS interface"},{"location":"inference/tfaas.html#tfaas-benchmarks","text":"Benchmark results on CentOS, 24 cores, 32GB of RAM serving DL NN with 42x128x128x128x64x64x1x1 architecture (JSON and ProtoBuffer formats show similar performance): - 400 req/sec for 100 concurrent clients, 1000 requests in total - 480 req/sec for 200 concurrent clients, 5000 requests in total For more information please visit bencmarks page.","title":"TFaaS benchmarks"},{"location":"inference/xgboost.html","text":"Direct inference with XGBoost \u00b6 General \u00b6 XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session). Existing Examples \u00b6 There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment. Example: Classification of points from joint-Gaussian distribution. \u00b6 In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv . Preparing Model \u00b6 The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details. C/C++ Usage with CMSSW \u00b6 To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following. Module setup \u00b6 There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so Basic Usage of C API \u00b6 In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */ Full Example \u00b6 Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample ) Python Usage \u00b6 To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section. Caveat \u00b6 It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ). Appendix: Tips for XGBoost users \u00b6 Importance Plot \u00b6 XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ). ROC Curve and AUC \u00b6 The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below Reference of XGBoost \u00b6 XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"XGBoost"},{"location":"inference/xgboost.html#direct-inference-with-xgboost","text":"","title":"Direct inference with XGBoost"},{"location":"inference/xgboost.html#general","text":"XGBoost is avaliable (at least) since CMSSW_9_2_4 cmssw#19377 . In CMSSW environment, XGBoost can be used via its Python API . For UL era, there are different verisons available for different SCRAM_ARCH : For slc7_amd64_gcc700 and above, ver.0.80 is available. For slc7_amd64_gcc900 and above, ver.1.3.3 is available. Please note that different major versions have different behavior( See Caveat Session).","title":"General"},{"location":"inference/xgboost.html#existing-examples","text":"There are some existing good examples of using XGBoost under CMSSW, as listed below: Offical sample for testing the integration of XGBoost library with CMSSW. Useful codes created by Dr. Huilin Qu for inference with existing trained model. C/C++ Interface for inference with existing trained model. We will provide examples for both C/C++ interface and python interface of XGBoost under CMSSW environment.","title":"Existing Examples"},{"location":"inference/xgboost.html#example-classification-of-points-from-joint-gaussian-distribution","text":"In this specific example, you will use XGBoost to classify data points generated from two 8-dimension joint-Gaussian distribution. Feature Index 0 1 2 3 4 5 6 7 \u03bc 1 1 2 3 4 5 6 7 8 \u03bc 2 0 1.9 3.2 4.5 4.8 6.1 8.1 11 \u03c3 \u00bd = \u03c3 1 1 1 1 1 1 1 1 |\u03bc 1 - \u03bc 2 | / \u03c3 1 0.1 0.2 0.5 0.2 0.1 1.1 3 All generated data points for train(1:10000,2:10000) and test(1:1000,2:1000) are stored as Train_data.csv / Test_data.csv .","title":"Example: Classification of points from joint-Gaussian distribution."},{"location":"inference/xgboost.html#preparing-model","text":"The training process of a XGBoost model can be done outside of CMSSW. We provide a python script for illustration. # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier # Or XGBRegressor for Logistic Regression import matplotlib.pyplot as plt import pandas as pd # specify parameters via map param = { 'n_estimators' : 50 } xgb = XGBClassifier ( param ) # using Pandas.DataFrame data-format, other available format are XGBoost's DMatrix and numpy.ndarray train_data = pd . read_csv ( \"path/to/the/data\" ) # The training dataset is code/XGBoost/Train_data.csv train_Variable = train_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] train_Score = train_data [ 'Type' ] # Score should be integer, 0, 1, (2 and larger for multiclass) test_data = pd . read_csv ( \"path/to/the/data\" ) # The testing dataset is code/XGBoost/Test_data.csv test_Variable = test_data [ '0' , '1' , '2' , '3' , '4' , '5' , '6' , '7' ] test_Score = test_data [ 'Type' ] # Now the data are well prepared and named as train_Variable, train_Score and test_Variable, test_Score. xgb . fit ( train_Variable , train_Score ) # Training xgb . predict ( test_Variable ) # Outputs are integers xgb . predict_proba ( test_Variable ) # Output scores , output structre: [prob for 0, prob for 1,...] xgb . save_model ( \"\\Path\\To\\Where\\You\\Want\\ModelName.model\" ) # Saving model The saved model ModelName.model is thus available for python and C/C++ api to load. Please use the XGBoost major version consistently (see Caveat ). While training with data from different datasets, proper treatment of weights are necessary for better model performance. Please refer to Official Recommendation for more details.","title":"Preparing Model"},{"location":"inference/xgboost.html#cc-usage-with-cmssw","text":"To use a saved XGBoost model with C/C++ code, it is convenient to use the XGBoost's offical C api . Here we provide a simple example as following.","title":"C/C++ Usage with CMSSW"},{"location":"inference/xgboost.html#module-setup","text":"There is no official CMSSW interface for XGBoost while its library are placed in cvmfs of CMSSW. Thus we have to use the raw c_api as well as setting up the library manually. To run XGBoost's c_api within CMSSW framework, in addition to the following standard setup. export SCRAM_ARCH = \"slc7_amd64_gcc700\" # To use higher version, please switch to slc7_amd64_900 export CMSSW_VERSION = \"CMSSW_X_Y_Z\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b The addtional effort is to add corresponding xml file(s) to $CMSSW_BASE/toolbox$CMSSW_BASE/config/toolbox/$SCRAM_ARCH/tools/selected/ for setting up XGBoost. For lower version (<1), add two xml files as below. xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/lib\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> <use name= \"rabit\" /> </tool> rabit.xml <tool name= \"rabit\" version= \"0.80\" > <client> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/py2-xgboost/0.80-ikaegh/lib/python2.7/site-packages/xgboost/rabit/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Please note that the path in cvmfs is not fixed, one can list all available versions in the py2-xgboost directory and choose one to use. For higher version (>=1), and one xml file xgboost.xml <tool name= \"xgboost\" version= \"0.80\" > <lib name= \"xgboost\" /> <client> <environment name= \"LIBDIR\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/lib64\" /> <environment name= \"INCLUDE\" default= \"/cvmfs/cms.cern.ch/$SCRAM_ARCH/external/xgboost/1.3.3/include/\" /> </client> <runtime name= \"ROOT_INCLUDE_PATH\" value= \"$INCLUDE\" type= \"path\" /> <runtime name= \"PATH\" value= \"$INCLUDE\" type= \"path\" /> </tool> Also one has the freedom to choose the available xgboost version inside xgboost directory. After adding xml file(s), the following commands should be executed for setting up. For lower version (<1), use scram setup rabit scram setup xgboost For higher version (>=1), use scram setup xgboost For using XGBoost as a plugin of CMSSW, it is necessary to add <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml . If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): <use name= \"xgboost\" /> <export> <lib name= \"1\" /> </export> The libxgboost.so would be too large to load for cmsRun job, please using the following commands for pre-loading: export LD_PRELOAD = $CMSSW_BASE /external/ $SCRAM_ARCH /lib/libxgboost.so","title":"Module setup"},{"location":"inference/xgboost.html#basic-usage-of-c-api","text":"In order to use c_api of XGBoost to load model and operate inference, one should construct necessaries objects: Files to include #include <xgboost/c_api.h> BoosterHandle : worker of XGBoost // Declare Object BoosterHandle booster_ ; // Allocate memory in C style XGBoosterCreate ( NULL , 0 , & booster_ ); // Load Model XGBoosterLoadModel ( booster_ , model_path . c_str ()); // second argument should be a const char *. DMatrixHandle : handle to dmatrix, the data format of XGBoost float TestData [ 2000 ][ 8 ] // Suppose 2000 data points, each data point has 8 dimension // Assign data to the \"TestData\" 2d array ... // Declare object DMatrixHandle data_ ; // Allocate memory and use external float array to initialize XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); // The first argument takes in float * namely 1d float array only, 2nd & 3rd: shape of input, 4th: value to replace missing ones XGBoosterPredict : function for inference bst_ulong outlen ; // bst_ulong is a typedef of unsigned long const float * f ; // array to store predictions XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); // lower version API // XGBoosterPredict(booster_,data_,0,0,0,&out_len,&f);// higher version API /* lower version (ver.<1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) higher version (ver.>=1) API XGB_DLL int XGBoosterPredict( BoosterHandle handle, DMatrixHandle dmat, int option_mask, // 0 for normal output, namely reporting scores int ntree_limit, // how many trees for prediction, set to 0 means no limit int training, // 0 for prediction bst_ulong * out_len, const float ** out_result ) */","title":"Basic Usage of C API"},{"location":"inference/xgboost.html#full-example","text":"Click to expand full example The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 XGBoostExample.cc \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 python/ \u2502 \u2514\u2500\u2500 xgboost_cfg.py \u2502 \u251c\u2500\u2500 toolbox/ (storing necessary xml(s) to be copied to toolbox/ of $CMSSW_BASE) \u2502 \u2514\u2500\u2500 xgboost.xml \u2502 \u2514\u2500\u2500 rabit.xml (lower version only) \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 Test_data.csv \u2514\u2500\u2500 lowVer.model / highVer.model Please also note that in order to operate inference in an event-by-event way, please put XGBoosterPredict in analyze rather than beginJob . plugins/XGBoostExample.cc for lower version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); cout << \"Hello World No.2\" << endl ; XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; cout << \"Hello World No.3\" << endl ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); cout << \"Hello World No.4\" << endl ; float TestData [ 2000 ][ 8 ]; cout << \"Hello World No.5\" << endl ; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } cout << \"Hello World No.6\" << endl ; DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); cout << \"Hello World No.7\" << endl ; bst_ulong out_len = 0 ; const float * f ; cout << out_len << endl ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , & out_len , & f ); cout << ret << endl ; for ( unsigned int i = 0 ; i < 2 ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; cout << \"Hello World No.8\" << endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for lower version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for lower version XGBoost plugins/XGBoostExample.cc for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 // -*- C++ -*- // // Package: XGB_Example/XGBoostExample // Class: XGBoostExample // /**\\class XGBoostExample XGBoostExample.cc XGB_Example/XGBoostExample/plugins/XGBoostExample.cc Description: [one line class summary] Implementation: [Notes on implementation] */ // // Original Author: Qian Sitian // Created: Sat, 19 Jun 2021 08:38:51 GMT // // // system include files #include <memory> // user include files #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"FWCore/Utilities/interface/InputTag.h\" #include \"DataFormats/TrackReco/interface/Track.h\" #include \"DataFormats/TrackReco/interface/TrackFwd.h\" #include <xgboost/c_api.h> #include <vector> #include <tuple> #include <string> #include <iostream> #include <fstream> #include <sstream> using namespace std ; vector < vector < double >> readinCSV ( const char * name ){ auto fin = ifstream ( name ); vector < vector < double >> floatVec ; string strFloat ; float fNum ; int counter = 0 ; getline ( fin , strFloat ); while ( getline ( fin , strFloat )) { std :: stringstream linestream ( strFloat ); floatVec . push_back ( std :: vector < double > ()); while ( linestream >> fNum ) { floatVec [ counter ]. push_back ( fNum ); if ( linestream . peek () == ',' ) linestream . ignore (); } ++ counter ; } return floatVec ; } // // class declaration // // If the analyzer does not use TFileService, please remove // the template argument to the base class so the class inherits // from edm::one::EDAnalyzer<> // This will improve performance in multithreaded jobs. class XGBoostExample : public edm :: one :: EDAnalyzer <> { public : explicit XGBoostExample ( const edm :: ParameterSet & ); ~ XGBoostExample (); static void fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ); private : virtual void beginJob () ; virtual void analyze ( const edm :: Event & , const edm :: EventSetup & ) ; virtual void endJob () ; // ----------member data --------------------------- std :: string test_data_path ; std :: string model_path ; }; // // constants, enums and typedefs // // // static data member definitions // // // constructors and destructor // XGBoostExample :: XGBoostExample ( const edm :: ParameterSet & config ) : test_data_path ( config . getParameter < std :: string > ( \"test_data_path\" )), model_path ( config . getParameter < std :: string > ( \"model_path\" )) { } XGBoostExample ::~ XGBoostExample () { // do anything here that needs to be done at desctruction time // (e.g. close files, deallocate resources etc.) } // // member functions // void XGBoostExample :: analyze ( const edm :: Event & iEvent , const edm :: EventSetup & iSetup ) { } void XGBoostExample :: beginJob () { BoosterHandle booster_ ; XGBoosterCreate ( NULL , 0 , & booster_ ); XGBoosterLoadModel ( booster_ , model_path . c_str ()); unsigned long numFeature = 0 ; vector < vector < double >> TestDataVector = readinCSV ( test_data_path . c_str ()); float TestData [ 2000 ][ 8 ]; for ( unsigned i = 0 ; ( i < 2000 ); i ++ ) { for ( unsigned j = 0 ; ( j < 8 ); j ++ ) { TestData [ i ][ j ] = TestDataVector [ i ][ j ]; // cout<<TestData[i][j]<<\"\\t\"; } //cout<<endl; } DMatrixHandle data_ ; XGDMatrixCreateFromMat (( float * ) TestData , 2000 , 8 , - 1 , & data_ ); bst_ulong out_len = 0 ; const float * f ; auto ret = XGBoosterPredict ( booster_ , data_ , 0 , 0 , 0 , & out_len , & f ); for ( unsigned int i = 0 ; i < out_len ; i ++ ) std :: cout << i << \" \\t \" << f [ i ] << std :: endl ; } void XGBoostExample :: endJob () { } void XGBoostExample :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { //The following says we do not know what parameters are allowed so do no validation // Please change this to state exactly what you do use, even if it is no parameters edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"test_data_path\" ); desc . add < std :: string > ( \"model_path\" ); descriptions . addWithDefaultLabel ( desc ); //Specify that only 'tracks' is allowed //To use, remove the default given above and uncomment below //ParameterSetDescription desc; //desc.addUntracked<edm::InputTag>(\"tracks\",\"ctfWithMaterialTracks\"); //descriptions.addDefault(desc); } //define this as a plug-in DEFINE_FWK_MODULE ( XGBoostExample ); plugins/BuildFile.xml for higher version XGBoost 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"DataFormats/TrackReco\" /> <use name= \"xgboost\" /> <flags EDM_PLUGIN= \"1\" /> python/xgboost_cfg.py for higher version XGBoost 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # setup minimal options #options = VarParsing(\"python\") #options.setDefault(\"inputFiles\", \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\") # noqa #options.parseArguments() # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring('file:/afs/cern.ch/cms/Tutorials/TWIKI_DATA/TTJets_8TeV_53X.root')) process . source = cms . Source ( \"EmptySource\" ) #process.source = cms.Source(\"PoolSource\", # fileNames=cms.untracked.vstring(options.inputFiles)) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) process . XGBoostExample = cms . EDAnalyzer ( \"XGBoostExample\" ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) #process.load(\"XGB_Example.XGBoostExample.XGBoostExample_cfi\") process . XGBoostExample . model_path = cms . string ( \"/Your/Path/data/highVer.model\" ) process . XGBoostExample . test_data_path = cms . string ( \"/Your/Path/data/Test_data.csv\" ) # define what to run in the path process . p = cms . Path ( process . XGBoostExample )","title":"Full Example"},{"location":"inference/xgboost.html#python-usage","text":"To use XGBoost's python interface, using the snippet below under CMSSW environment # importing necessary models import numpy as np import pandas as pd from xgboost import XGBClassifier import matplotlib.pyplot as plt import pandas as pd xgb = XGBClassifier () xgb . load_model ( 'ModelName.model' ) # After loading model, usage is the same as discussed in the model preparation section.","title":"Python Usage"},{"location":"inference/xgboost.html#caveat","text":"It is worth mentioning that both behavior and APIs of different XGBoost version can have difference. When using c_api for C/C++ inference, for ver.<1 , the API is XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, int training, bst_ulong * out_len,const float ** out_result) , while for ver.>=1 the API changes to XGB_DLL int XGBoosterPredict(BoosterHandle handle, DMatrixHandle dmat,int option_mask, unsigned int ntree_limit, int training, bst_ulong * out_len,const float ** out_result) . Model from ver.>=1 cannot be used for ver.<1 . Other important issue for C/C++ user is that DMatrix only takes in single precision floats ( float ), not double precision floats ( double ).","title":"Caveat"},{"location":"inference/xgboost.html#appendix-tips-for-xgboost-users","text":"","title":"Appendix: Tips for XGBoost users"},{"location":"inference/xgboost.html#importance-plot","text":"XGBoost uses F-score to describe feature importance quantatitively. XGBoost's python API provides a nice tool, plot_importance , to plot the feature importance conveniently after finishing train . # Once the training is done, the plot_importance function can thus be used to plot the feature importance. from xgboost import plot_importance # Import the function plot_importance ( xgb ) # suppose the xgboost object is named \"xgb\" plt . savefig ( \"importance_plot.pdf\" ) # plot_importance is based on matplotlib, so the plot can be saved use plt.savefig() The importance plot is consistent with our expectation, as in our toy-model, the data points differ by most on the feature \"7\". (see toy model setup ).","title":"Importance Plot"},{"location":"inference/xgboost.html#roc-curve-and-auc","text":"The receiver operating characteristic (ROC) and auccrency (AUC) are key quantities to describe the model performance. For XGBoost, ROC curve and auc score can be easily obtained with the help of sci-kit learn (sklearn) functionals, which is also in CMSSW software. from sklearn.metrics import roc_auc_score , roc_curve , auc # ROC and AUC should be obtained on test set # Suppose the ground truth is 'y_test', and the output score is named as 'y_score' fpr , tpr , _ = roc_curve ( y_test , y_score ) roc_auc = auc ( fpr , tpr ) plt . figure () lw = 2 plt . plot ( fpr , tpr , color = 'darkorange' , lw = lw , label = 'ROC curve (area = %0.2f )' % roc_auc ) plt . plot ([ 0 , 1 ], [ 0 , 1 ], color = 'navy' , lw = lw , linestyle = '--' ) plt . xlim ([ 0.0 , 1.0 ]) plt . ylim ([ 0.0 , 1.05 ]) plt . xlabel ( 'False Positive Rate' ) plt . ylabel ( 'True Positive Rate' ) plt . title ( 'Receiver operating characteristic example' ) plt . legend ( loc = \"lower right\" ) # plt.show() # display the figure when not using jupyter display plt . savefig ( \"roc.png\" ) # resulting plot is shown below","title":"ROC Curve and AUC"},{"location":"inference/xgboost.html#reference-of-xgboost","text":"XGBoost Wiki: https://en.wikipedia.org/wiki/XGBoost XGBoost Github Repo.: https://github.com/dmlc/xgboost XGBoost offical api tutorial Latest, Python: https://xgboost.readthedocs.io/en/latest/python/index.html Latest, C/C++: https://xgboost.readthedocs.io/en/latest/tutorials/c_api_tutorial.html Older (0.80), Python: https://xgboost.readthedocs.io/en/release_0.80/python/index.html No Tutorial for older version C/C++ api, source code: https://github.com/dmlc/xgboost/blob/release_0.80/src/c_api/c_api.cc","title":"Reference of XGBoost"},{"location":"optimization/data_augmentation.html","text":"Todo.","title":"Data augmentation"},{"location":"optimization/importance.html","text":"Todo. Idea: Methods to determine feature importance Sources: Javier Giles - Perturbation ranking Roger Wolf - 1803.08782 Identifying the relevant dependencies of the neural network response on characteristics of the input space https://arxiv.org/abs/1808.04260 6 iNNvestigate neural networks! https://arxiv.org/abs/1803.08782 4","title":"Feature importance"},{"location":"optimization/introduction.html","text":"Model optimization \u00b6 Todo.","title":"Model optimization"},{"location":"optimization/introduction.html#model-optimization","text":"Todo.","title":"Model optimization"},{"location":"validation/cross_validation.html","text":"How to do cross validation Other sources of information","title":"Cross validation"},{"location":"validation/overtraining.html","text":"In progress... Clear examples of overtraining, how to spot it, and how to prevent it.","title":"Overtraining"}]}