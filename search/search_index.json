{"config":{"lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Content.","title":"Home"},{"location":"inference/checklist.html","text":"Integration checklist \u00b6 Todo.","title":"Integration checklist"},{"location":"inference/checklist.html#integration-checklist","text":"Todo.","title":"Integration checklist"},{"location":"inference/hls4ml.html","text":"Direct inference with hls4ml \u00b6 Todo.","title":"hls4ml"},{"location":"inference/hls4ml.html#direct-inference-with-hls4ml","text":"Todo.","title":"Direct inference with hls4ml"},{"location":"inference/integrations.html","text":"Successful integrations \u00b6 Todo.","title":"Successful integrations"},{"location":"inference/integrations.html#successful-integrations","text":"Todo.","title":"Successful integrations"},{"location":"inference/onnx.html","text":"Direct inference with ONNX \u00b6 Todo.","title":"ONNX"},{"location":"inference/onnx.html#direct-inference-with-onnx","text":"Todo.","title":"Direct inference with ONNX"},{"location":"inference/performance.html","text":"Performance of inference tools \u00b6","title":"Performance"},{"location":"inference/performance.html#performance-of-inference-tools","text":"","title":"Performance of inference tools"},{"location":"inference/sonic_triton.html","text":"Service-based inference with Triton/Sonic \u00b6 Todo.","title":"Sonic/Triton"},{"location":"inference/sonic_triton.html#service-based-inference-with-tritonsonic","text":"Todo.","title":"Service-based inference with Triton/Sonic"},{"location":"inference/tensorflow1.html","text":"Direct inference with TensorFlow 1 \u00b6 Todo.","title":"TensorFlow 1"},{"location":"inference/tensorflow1.html#direct-inference-with-tensorflow-1","text":"Todo.","title":"Direct inference with TensorFlow 1"},{"location":"inference/tensorflow2.html","text":"Direct inference with TensorFlow 2 \u00b6 TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.2. See the guide on inference with TensorFlow 1 for earlier versions. Software setup \u00b6 To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml Saving your model \u00b6 After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph . Inference in CMSSW \u00b6 The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended . CMSSW module setup \u00b6 If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export> Single-threaded inference \u00b6 Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ... 2. Initialize objects \u00b6 1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef ); 3. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 4. Cleanup \u00b6 1 2 tensorflow :: closeSession ( session ); delete graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addDefault ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Multi-threaded inference \u00b6 Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup. 1. Includes \u00b6 1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h . 2. Define and use the cache \u00b6 The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details. 3. Initialize objects \u00b6 1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef ); 4. Inference \u00b6 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float 5. Cleanup \u00b6 1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ; Full example \u00b6 Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addDefault ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin ) Optimization \u00b6 Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist . Reusing tensors \u00b6 In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call. Tensor data access via pointers \u00b6 As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } } Inter- and intra-operation parallelism \u00b6 Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" ); Miscellaneous \u00b6 Logging \u00b6 By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet. Links and further reading \u00b6 cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API","title":"TensorFlow 2"},{"location":"inference/tensorflow2.html#direct-inference-with-tensorflow-2","text":"TensorFlow 2 is available since CMSSW_11_1_X ( cmssw#28711 , cmsdist#5525 ). The integration into the software stack can be found in cmsdist/tensorflow.spec and the interface is located in cmssw/PhysicsTools/TensorFlow . The current version is 2.1.0 and, at the moment, only supports inference on CPU. GPU support is planned for the integration of version 2.2. See the guide on inference with TensorFlow 1 for earlier versions.","title":"Direct inference with TensorFlow 2"},{"location":"inference/tensorflow2.html#software-setup","text":"To run the examples shown below, create a mininmal inference setup with the following snippet. 1 2 3 4 5 6 7 8 9 10 export SCRAM_ARCH = \"slc7_amd64_gcc820\" export CMSSW_VERSION = \"CMSSW_11_1_2\" source /cvmfs/cms.cern.ch/cmsset_default.sh cmsrel \" $CMSSW_VERSION \" cd \" $CMSSW_VERSION /src\" cmsenv scram b Below, the cmsml Python package is used to convert models from TensorFlow objects ( tf.function 's or Keras models) to protobuf graph files ( documentation ). It should be available after executing the commands above. You can check its version via python -c \"import cmsml; print(cmsml.__version__)\" and compare to the released tags . If you want to install a newer version from either the master branch of the cmsml repository or the Python package index (PyPI) , you can simply do that via pip. master # into your user directory (usually ~/.local) pip install --upgrade --user git+https://github.com/cms-ml/cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" git+https://github.com/cms-ml/cmsml PyPI # into your user directory (usually ~/.local) pip install --upgrade --user cmsml # _or_ # into a custom directory pip install --upgrade --prefix \"CUSTOM_DIRECTORY\" cmsml","title":"Software setup"},{"location":"inference/tensorflow2.html#saving-your-model","text":"After successfully training, you should save your model in a protobuf graph file which can be read by the interface in CMSSW. Naturally, you only want to save that part of your model is required to run the network prediction, i.e., it should not contain operations related to model training or loss functions (unless explicitely required). Also, to reduce the memory footprint and to accelerate the inference, variables should be converted to constant tensors. Both of these model transformations are provided by the cmsml package. Instructions on how to transform and save your model are shown below, depending on whether you use Keras or plain TensorFlow with tf.function 's. Keras The code below saves a Keras Model instance as a protobuf graph file using cmsml.tensorflow.save_graph . In order for Keras to built the internal graph representation before saving, make sure to either compile the model, or pass an input_shape to the first layer: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # coding: utf-8 import tensorflow as tf import tf.keras.layers as layers import cmsml # define your model model = tf . keras . Sequential () model . add ( layers . InputLayer ( input_shape = ( 10 ,), name = \"input\" )) model . add ( layers . Dense ( 100 , activation = \"tanh\" )) model . add ( layers . Dense ( 3 , activation = \"softmax\" , name = \"output\" )) # train it ... # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , model , variables_to_constants = True ) Following the Keras naming conventions for certain layers, the input will be named \"input\" while the output is named \"sequential/output/Softmax\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . tf.function Let's consider you write your network model in a single tf.function . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # coding: utf-8 import tensorflow as tf import cmsml # define the model @tf . function def model ( x ): # lift variable initialization to the lowest context so they are # not re-initialized on every call (eager calls or signature tracing) with tf . init_scope (): W = tf . Variable ( tf . ones ([ 10 , 1 ])) b = tf . Variable ( tf . ones ([ 1 ])) # define your \"complex\" model here h = tf . add ( tf . matmul ( x , W ), b ) y = tf . tanh ( h , name = \"y\" ) return y In TensorFlow terms, the model function is polymorphic - it accepts different types of the input tensor x ( tf.float32 , tf.float64 , ...). For each type, TensorFlow will create a concrete function with an associated tf.Graph object. This mechanism is referred to as signature tracing . For deeper insights into tf.function , the concepts of signature tracing, polymorphic and concrete functions, see the guide on Better performance with tf.function . To save the model as a protobuf graph file, you explicitely need to create a concrete function. However, this is fairly easy once you know the exact type and shape of all input arguments. 20 21 22 23 24 25 26 # create a concrete function cmodel = model . get_concrete_function ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 )) # convert to binary (.pb extension) protobuf # with variables converted to constants cmsml . tensorflow . save_graph ( \"graph.pb\" , cmodel , variables_to_constants = True ) The input will be named \"x\" while the output is named \"y\" . To cross check the names, you can save the graph in text format by using the extension \".pb.txt\" . Different method: Frozen signatures Instead of creating a polymorphic tf.function and extracting a concrete one in a second step, you can directly define an input signature upon definition. @tf . function ( input_signature = ( tf . TensorSpec ( shape = [ 2 , 10 ], dtype = tf . float32 ),)) def model ( x ): ... This disables signature tracing since the input signature is frozen. However, you can directly pass it to cmsml.tensorflow.save_graph .","title":"Saving your model"},{"location":"inference/tensorflow2.html#inference-in-cmssw","text":"The inference can be implemented to run in a single thread . In general, this does not mean that the module cannot be executed with multiple threads ( cmsRun --numThreads <N> <CFG_FILE> ), but rather that its performance in terms of evaluation time and especially memory consumption is likely to be suboptimal. Therefore, for modules to be integrated into CMSSW, the multi-threaded implementation is strongly recommended .","title":"Inference in CMSSW"},{"location":"inference/tensorflow2.html#cmssw-module-setup","text":"If you aim to use the TensorFlow interface in a CMSSW plugin , make sure to include 1 2 3 <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> in your plugins/BuildFile.xml file. If you are using the interface inside the src/ or interface/ directory of your module, make sure to create a global BuildFile.xml file next to theses directories, containing (at least): 1 2 3 4 5 <use name= \"PhysicsTools/TensorFlow\" /> <export> <lib name= \"1\" /> </export>","title":"CMSSW module setup"},{"location":"inference/tensorflow2.html#single-threaded-inference","text":"Despite tf.Session being removed in the Python interface as of TensorFlow 2, the concepts of Graph 's, containing the constant computational structure and trained variables of your model, Session 's, handling execution, data exchange and device placement, and the separation between them lives on in the C++ interface. Thus, the overall inference approach is 1) include the interface, 2) initialize Graph and session , 3) per event create input tensors and run the inference, and 4) cleanup.","title":"Single-threaded inference"},{"location":"inference/tensorflow2.html#1-includes","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" // further framework includes ...","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-initialize-objects","text":"1 2 3 4 5 6 7 8 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // load the graph definition tensorflow :: GraphDef * graphDef = tensorflow :: loadGraphDef ( \"/path/to/constantgraph.pb\" ); // create a session tensorflow :: Session * session = tensorflow :: createSession ( graphDef );","title":"2. Initialize objects"},{"location":"inference/tensorflow2.html#3-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { \"input\" , input } }, { \"output\" }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"3. Inference"},{"location":"inference/tensorflow2.html#4-cleanup","text":"1 2 tensorflow :: closeSession ( session ); delete graphDef ;","title":"4. Cleanup"},{"location":"inference/tensorflow2.html#full-example","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 /* * Example plugin to demonstrate the direct single-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/one/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" class MyPlugin : public edm :: one :: EDAnalyzer <> { public : explicit MyPlugin ( const edm :: ParameterSet & ); ~ MyPlugin (); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string graphPath_ ; std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: GraphDef * graphDef_ ; tensorflow :: Session * session_ ; }; void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addDefault ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config ) : graphPath_ ( config . getParameter < std :: string > ( \"graphPath\" )), inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), graphDef_ ( nullptr ), session_ ( nullptr ) { // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); } void MyPlugin :: beginJob () { // load the graph graphDef_ = tensorflow :: loadGraphDef ( graphPath_ ); // create a new session and add the graphDef session_ = tensorflow :: createSession ( graphDef_ ); } void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); // delete the graph delete graphDef_ ; graphDef_ = nullptr ; } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#multi-threaded-inference","text":"Compared to the single-threaded implementation above , the multi-threaded version has one major difference: the Graph is no longer a member of a particular module instance, but rather shared between all instances in all threads . This is possible since the Graph is actually a constant object that does not change over the course of the inference process. All volatile, device dependent information is kept in a Session which we keep instantiating per module instance. The Graph on the other hand is stored in a edm :: GlobalCache < T > . See the documentation on the C++ interface of stream modules for details. Thus, the overall inference approach is 1) include the interface, 2) define the edm :: GlobalCache < T > holding the Graph , 3) initialize the Session with the cached Graph , 4) per event create input tensors and run the inference, and 5) cleanup.","title":"Multi-threaded inference"},{"location":"inference/tensorflow2.html#1-includes_1","text":"1 2 3 4 #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" // further framework includes ... Note that stream/EDAnalyzer.h is included rather than one/EDAnalyzer.h .","title":"1. Includes"},{"location":"inference/tensorflow2.html#2-define-and-use-the-cache","text":"The cache definition is done by declaring a simle struct. 1 2 3 4 struct MyCache { MyCache () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; Use it in the edm :: GlobalCache template argument and adjust the plugin accordingly. 1 2 3 4 5 6 7 8 9 class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit GraphLoadingMT ( const edm :: ParameterSet & , const CacheData * ); ~ GraphLoadingMT (); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); ... Implement initializeGlobalCache and globalEndJob to control the behavior of how the cache object is created and destroyed. See the full example below for details.","title":"2. Define and use the cache"},{"location":"inference/tensorflow2.html#3-initialize-objects","text":"1 2 3 4 5 // configure logging to show warnings (see table below) tensorflow :: setLogging ( \"2\" ); // create a session using the graphDef stored in the cache tensorflow :: Session * session = tensorflow :: createSession ( cacheData -> graphDef );","title":"3. Initialize objects"},{"location":"inference/tensorflow2.html#4-inference","text":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // create an input tensor // (example: single batch of 10 values) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); // fill the tensor with your input data // (example: just fill consecutive values) for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // run the evaluation std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs ); // process the output tensor // (example: print the 5th value of the 0th (the only) example) std :: cout << outputs [ 0 ]. matrix < float > ()( 0 , 5 ) << std :: endl ; // -> float","title":"4. Inference"},{"location":"inference/tensorflow2.html#5-cleanup","text":"1 2 3 4 5 // per module instance tensorflow :: closeSession ( session_ ); // in globalEndJob delete cacheData -> graphDef ;","title":"5. Cleanup"},{"location":"inference/tensorflow2.html#full-example_1","text":"Click to expand The example assumes the following directory structure: MySubsystem/MyModule/ \u2502 \u251c\u2500\u2500 plugins/ \u2502 \u251c\u2500\u2500 MyPlugin.cpp \u2502 \u2514\u2500\u2500 BuildFile.xml \u2502 \u251c\u2500\u2500 test/ \u2502 \u2514\u2500\u2500 my_plugin_cfg.py \u2502 \u2514\u2500\u2500 data/ \u2514\u2500\u2500 graph.pb plugins/MyPlugin.cpp 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 /* * Example plugin to demonstrate the direct multi-threaded inference with TensorFlow 2. */ #include <memory> #include \"FWCore/Framework/interface/Event.h\" #include \"FWCore/Framework/interface/Frameworkfwd.h\" #include \"FWCore/Framework/interface/MakerMacros.h\" #include \"FWCore/Framework/interface/stream/EDAnalyzer.h\" #include \"FWCore/ParameterSet/interface/ParameterSet.h\" #include \"PhysicsTools/TensorFlow/interface/TensorFlow.h\" // define the cache object // it could handle graph loading and destruction on its own, // but in this example, we define it as a logicless container struct CacheData { CacheData () : graphDef ( nullptr ) {} std :: atomic < tensorflow :: GraphDef *> graphDef ; }; class MyPlugin : public edm :: stream :: EDAnalyzer < edm :: GlobalCache < CacheData >> { public : explicit MyPlugin ( const edm :: ParameterSet & , const CacheData * ); ~ MyPlugin (); static void fillDescriptions ( edm :: ConfigurationDescriptions & ); // two additional static methods for handling the global cache static std :: unique_ptr < CacheData > initializeGlobalCache ( const edm :: ParameterSet & ); static void globalEndJob ( const CacheData * ); private : void beginJob (); void analyze ( const edm :: Event & , const edm :: EventSetup & ); void endJob (); std :: string inputTensorName_ ; std :: string outputTensorName_ ; tensorflow :: Session * session_ ; }; std :: unique_ptr < CacheData > MyPlugin :: initializeGlobalCache ( const edm :: ParameterSet & config ) { // this method is supposed to create, initialize and return a CacheData instance CacheData * cacheData = new CacheData (); // load the graph def and save it std :: string graphPath = config . getParameter < std :: string > ( \"graphPath\" ); cacheData -> graphDef = tensorflow :: loadGraphDef ( graphPath ); // set tensorflow log leven to warning tensorflow :: setLogging ( \"2\" ); return std :: unique_ptr < CacheData > ( cacheData ); } void MyPlugin :: globalEndJob ( const CacheData * cacheData ) { // reset the graphDef if ( cacheData -> graphDef != nullptr ) { delete cacheData -> graphDef ; } } void MyPlugin :: fillDescriptions ( edm :: ConfigurationDescriptions & descriptions ) { // defining this function will lead to a *_cfi file being generated when compiling edm :: ParameterSetDescription desc ; desc . add < std :: string > ( \"graphPath\" ); desc . add < std :: string > ( \"inputTensorName\" ); desc . add < std :: string > ( \"outputTensorName\" ); descriptions . addDefault ( desc ); } MyPlugin :: MyPlugin ( const edm :: ParameterSet & config , const CacheData * cacheData ) : inputTensorName_ ( config . getParameter < std :: string > ( \"inputTensorName\" )), outputTensorName_ ( config . getParameter < std :: string > ( \"outputTensorName\" )), session_ ( tensorflow :: createSession ( cacheData -> graphDef )) {} void MyPlugin :: beginJob () {} void MyPlugin :: endJob () { // close the session tensorflow :: closeSession ( session_ ); } void MyPlugin :: analyze ( const edm :: Event & event , const edm :: EventSetup & setup ) { // define a tensor and fill it with range(10) tensorflow :: Tensor input ( tensorflow :: DT_FLOAT , { 1 , 10 }); for ( size_t i = 0 ; i < 10 ; i ++ ) { input . matrix < float > ()( 0 , i ) = float ( i ); } // define the output and run std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session_ , {{ inputTensorName_ , input }}, { outputTensorName_ }, & outputs ); // print the output std :: cout << \" -> \" << outputs [ 0 ]. matrix < float > ()( 0 , 0 ) << std :: endl << std :: endl ; } DEFINE_FWK_MODULE ( MyPlugin ); plugins/BuildFile.xml 1 2 3 4 5 6 <use name= \"FWCore/Framework\" /> <use name= \"FWCore/PluginManager\" /> <use name= \"FWCore/ParameterSet\" /> <use name= \"PhysicsTools/TensorFlow\" /> <flags EDM_PLUGIN= \"1\" /> test/my_plugin_cfg.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # coding: utf-8 import os import FWCore.ParameterSet.Config as cms from FWCore.ParameterSet.VarParsing import VarParsing # get the data/ directory thisdir = os . path . dirname ( os . path . abspath ( __file__ )) datadir = os . path . join ( os . path . dirname ( thisdir ), \"data\" ) # setup minimal options options = VarParsing ( \"python\" ) options . setDefault ( \"inputFiles\" , \"root://xrootd-cms.infn.it//store/mc/RunIIFall17MiniAOD/DYJetsToLL_M-10to50_TuneCP5_13TeV-madgraphMLM-pythia8/MINIAODSIM/94X_mc2017_realistic_v10-v2/00000/9A439935-1FFF-E711-AE07-D4AE5269F5FF.root\" ) # noqa options . parseArguments () # define the process to run process = cms . Process ( \"TEST\" ) # minimal configuration process . load ( \"FWCore.MessageService.MessageLogger_cfi\" ) process . MessageLogger . cerr . FwkReport . reportEvery = 1 process . maxEvents = cms . untracked . PSet ( input = cms . untracked . int32 ( 10 )) process . source = cms . Source ( \"PoolSource\" , fileNames = cms . untracked . vstring ( options . inputFiles )) # process options process . options = cms . untracked . PSet ( allowUnscheduled = cms . untracked . bool ( True ), wantSummary = cms . untracked . bool ( True ), ) # setup MyPlugin by loading the auto-generated cfi (see MyPlugin.fillDescriptions) process . load ( \"MySubsystem.MyModule.myPlugin_cfi\" ) process . myPlugin . graphPath = cms . string ( os . path . join ( datadir , \"graph.pb\" )) process . myPlugin . inputTensorName = cms . string ( \"input\" ) process . myPlugin . outputTensorName = cms . string ( \"output\" ) # define what to run in the path process . p = cms . Path ( process . myPlugin )","title":"Full example"},{"location":"inference/tensorflow2.html#optimization","text":"Depending on the use case, the following approaches can optimize the inference performance. It could be worth checking them out in your algorithm. Further optimization approaches can be found in the integration checklist .","title":"Optimization"},{"location":"inference/tensorflow2.html#reusing-tensors","text":"In some cases, instead of creating new input tensors for each inference call, you might want to store input tensors as members of your plugin. This is of course possible if you know its exact shape a-prioro and comes with the cost of keeping the tensor in memory for the lifetime of your module instance. You can use tensor . flat < float > (). setZero (); to reset the values of your tensor prior to each call.","title":"Reusing tensors"},{"location":"inference/tensorflow2.html#tensor-data-access-via-pointers","text":"As shown in the examples above, tensor data can be accessed through methods such as flat<type>() or matrix<type>() which return objects that represent the underlying data in the requested structure ( tensorflow::Tensor C++ API ). To read and manipulate particular elements, you can directly call this object with the coordinates of an element. // matrix returns a 2D representation // set element (b,i) to f tensor . matrix < float > ()( b , i ) = float ( f ); However, doing this for a large input tensor might entail some overhead. Since the data is actually contiguous in memory (C-style \"row-major\" memory ordering), a faster (though less explicit) way of interacting with tensor data is using a pointer. // get the pointer to the first tensor element float * d = tensor . flat < float > (). data (); Now, the tensor data can be filled using simple and fast pointer arithmetic. // fill tensor data using pointer arithmethic // memory ordering is row-major, so the most outer loop corresponds dimension 0 for ( size_t b = 0 ; b < batchSize ; b ++ ) { for ( size_t i = 0 ; i < nFeatures ; i ++ , d ++ ) { // note the d++ * d = float ( i ); } }","title":"Tensor data access via pointers"},{"location":"inference/tensorflow2.html#inter-and-intra-operation-parallelism","text":"Debugging and local processing only Parallelism between (inter) and within (intra) operations can greatly improve the inference performance. However, this allows TensorFlow to manage and schedule threads on its own, possibly interfering with the thread model inherent to CMSSW. For inference code that is to be officially integrated, you should avoid inter- and intra-op parallelism and rather adhere to the examples shown above. You can configure the amount of inter- and infra-op threads via the second argument of the tensorflow :: createSession method. Simple 1 tensorflow :: Session * session = tensorflow :: createSession ( graphDef , nThreads ); Verbose 1 2 3 4 5 tensorflow :: SessionOptions sessionOptions ; sessionOptions . config . set_intra_op_parallelism_threads ( nThreads ); sessionOptions . config . set_inter_op_parallelism_threads ( nThreads ); tensorflow :: Session * session = tensorflow :: createSession ( graphDef , sessionOptions ); Then, when calling tensorflow :: run , pass the internal name of the TensorFlow threadpool, i.e. \"tensorflow\" , as the last argument. 1 2 3 4 5 6 7 8 std :: vector < tensorflow :: Tensor > outputs ; tensorflow :: run ( session , { { inputTensorName , input } }, { outputTensorName }, & outputs , \"tensorflow\" );","title":"Inter- and intra-operation parallelism"},{"location":"inference/tensorflow2.html#miscellaneous","text":"","title":"Miscellaneous"},{"location":"inference/tensorflow2.html#logging","text":"By default, TensorFlow logging is quite verbose. This can be changed by either setting the TF_CPP_MIN_LOG_LEVEL environment varibale before calling cmsRun , or within your code through tensorflow :: setLogging ( level ) . Verbosity level TF_CPP_MIN_LOG_LEVEL debug \"0\" info \"1\" (default) warning \"2\" error \"3\" none \"4\" Forwarding logs to the MessageLogger service is not possible yet.","title":"Logging"},{"location":"inference/tensorflow2.html#links-and-further-reading","text":"cmsml package CMSSW TensorFlow interface documentation TensorFlow interface header CMSSW process options C++ interface of stream modules TensorFlow TensorFlow 2 tutorial tf.function C++ API tensorflow::Tensor tensorflow::Operation tensorflow::ClientSession Keras API","title":"Links and further reading"},{"location":"inference/xgboost.html","text":"Direct inference with XGBoost \u00b6 Todo.","title":"XGBoost"},{"location":"inference/xgboost.html#direct-inference-with-xgboost","text":"Todo.","title":"Direct inference with XGBoost"},{"location":"optimization/introduction.html","text":"Model optimization \u00b6 Todo.","title":"Model optimization"},{"location":"optimization/introduction.html#model-optimization","text":"Todo.","title":"Model optimization"},{"location":"starter/introduction.html","text":"Starter guide introdction \u00b6 Todo.","title":"Introduction"},{"location":"starter/introduction.html#starter-guide-introdction","text":"Todo.","title":"Starter guide introdction"}]}